{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project Notebook\n",
    "\n",
    "### NOTE: If you build and run a notebook in the cloud, just copy it down in place of this one!  \n",
    "#### Be sure to have all your output captured within the notebook!\n",
    "#### <span style=\"background:yellow\">Be sure to save your work early and often!</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Specific_Project_1.png MISSING](../images/Specific_Project_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add code as needed in the cells below to produce your analytical products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my project, I plan to look at the subreddit /r/unpopularopinion and see what kind of topics/opinions are commonly brought up. Are there 'popular' unpopular opinions that are posted?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of packages you must \"pip install\" on the console in order to run below\n",
    "\n",
    "google"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install --upgrade google-cloud-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Include whichever API are appropriate for your cloud provider\n",
    "\n",
    "#----------------------------Reddit RSS Feed----------------------------------------\n",
    "# For my project, I will run the RSS feed on the subreddit \n",
    "# r/unpopularopinion/\n",
    "#-----------------------------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "from google.cloud import storage\n",
    "## Change to your own project name\n",
    "PROJECT='lcmhng-mu-dsa'\n",
    "\n",
    "def list_blobs(bucket_name):\n",
    "    \"\"\"Lists all the blobs in the bucket.\"\"\"\n",
    "    storage_client = storage.Client(project=PROJECT)\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "\n",
    "    blobs = bucket.list_blobs()\n",
    "\n",
    "    for blob in blobs:\n",
    "        print(blob.name)\n",
    "\n",
    "def list_blobs_with_prefix(bucket_name, prefix, delimiter=None):\n",
    "    \"\"\"Lists all the blobs in the bucket that begin with the prefix.\n",
    "    This can be used to list all blobs in a \"folder\", e.g. \"public/\".\n",
    "    The delimiter argument can be used to restrict the results to only the\n",
    "    \"files\" in the given \"folder\". Without the delimiter, the entire tree under\n",
    "    the prefix is returned. For example, given these blobs:\n",
    "        /a/1.txt\n",
    "        /a/b/2.txt\n",
    "    If you just specify prefix = '/a', you'll get back:\n",
    "        /a/1.txt\n",
    "        /a/b/2.txt\n",
    "    However, if you specify prefix='/a' and delimiter='/', you'll get back:\n",
    "        /a/1.txt\n",
    "    \"\"\"\n",
    "    storage_client = storage.Client(project=PROJECT)\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "\n",
    "    blobs = bucket.list_blobs(prefix=prefix, delimiter=delimiter)\n",
    "\n",
    "    print('Blobs:')\n",
    "    for blob in blobs:\n",
    "        print(blob.name)\n",
    "\n",
    "    if delimiter:\n",
    "        print('Prefixes:')\n",
    "        for prefix in blobs.prefixes:\n",
    "            print(prefix)\n",
    "\n",
    "#------------------------------------------------------------------------UPLOAD----------------------------\n",
    "\n",
    "def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "    storage_client = storage.Client(project=PROJECT)\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "\n",
    "    print('File {} uploaded to {}.'.format(\n",
    "        source_file_name,\n",
    "        destination_blob_name))\n",
    "\n",
    "\n",
    "def upload_as_blob(bucket_name, source_data, destination_blob_name, content_type='text/plain'):\n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "    storage_client = storage.Client(project=PROJECT)\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    #blob.upload_from_filename(source_file_name)\n",
    "    blob.upload_from_string(source_data, content_type=content_type)\n",
    "    \n",
    "    print('Data uploaded to {}.'.format(destination_blob_name))\n",
    "\n",
    "def download_blob(bucket_name, source_blob_name, destination_file_name):\n",
    "    \"\"\"Downloads a blob from the bucket.\"\"\"\n",
    "    storage_client = storage.Client(project=PROJECT)\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(source_blob_name)\n",
    "\n",
    "    blob.download_to_filename(destination_file_name)\n",
    "\n",
    "    print('Blob {} downloaded to {}.'.format(\n",
    "        source_blob_name,\n",
    "        destination_file_name))\n",
    "\n",
    "\n",
    "def delete_blob(bucket_name, blob_name):\n",
    "    \"\"\"Deletes a blob from the bucket.\"\"\"\n",
    "    storage_client = storage.Client(project=PROJECT)\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "\n",
    "    blob.delete()\n",
    "\n",
    "    print('Blob {} deleted.'.format(blob_name))\n",
    "\n",
    "#-------------------------RUNTIME CODE\n",
    "\n",
    "# Bucket Name:\n",
    "#my_bucket_name = 'dsa_mini_project_lcmhng'\n",
    "my_bucket_name = 'dsa_project_lcmhng'\n",
    "\n",
    "list_blobs(my_bucket_name)\n",
    "\n",
    "#------------------------------Update with REDDIT download\n",
    "\n",
    "def read_blob(bucket_name, source_blob_name):\n",
    "    \"\"\"Downloads a blob from the bucket and returns content in bytearray\"\"\"\n",
    "    storage_client = storage.Client(project=PROJECT)\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(source_blob_name)\n",
    "    return blobdownload_as_string()\n",
    "\n",
    "def read_blob_as_string(bucket_name, source_blob_name):\n",
    "    \"\"\"Downloads a blob from the bucket and returns content in string\"\"\"\n",
    "    storage_client = storage.Client(project=PROJECT)\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(source_blob_name)\n",
    "    return blob.download_as_string().decode(\"utf-8\") \n",
    "\n",
    "#-----------------------------------CODE TO GRAB AND STORE REDDIT DATA\n",
    "\n",
    "import feedparser\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Functions from: https://stackoverflow.com/questions/1936466/beautifulsoup-grab-visible-webpage-text\n",
    "\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    return u\" \".join(t.strip() for t in visible_texts)\n",
    "\n",
    "# Define URL of the RSS Feed I want\n",
    "# Here we will update to the specific subreddit of choice r/unpopularopinion/\n",
    "\n",
    "a_reddit_rss_url = 'http://www.reddit.com/r/unpopularopinion/.rss?sort=new'\n",
    "\n",
    "feed = feedparser.parse( a_reddit_rss_url )\n",
    "\n",
    "\"\"\"\n",
    "if (feed['bozo'] == 1):\n",
    "    print(\"Error Reading/Parsing Feed XML Data\")    \n",
    "else:\n",
    "    for item in feed[ \"items\" ]:\n",
    "        dttm = item[ \"date\" ]\n",
    "        title = item[ \"title\" ]\n",
    "        summary_text = text_from_html(item[ \"summary\" ])\n",
    "        link = item[ \"link\" ]\n",
    "        \n",
    "        print(\"====================\")\n",
    "        print(\"Title: {} ({})\\nTimestamp: {}\".format(title,link,dttm))\n",
    "        print(\"--------------------\\nSummary:\\n{}\".format(summary_text))\n",
    "   \n",
    "\"\"\"\n",
    "              \n",
    "# ------------ Create file string\n",
    "\"\"\"         \n",
    "def reddit_post_string(feed):\n",
    "\"\"\"\n",
    "    #Funciton to generate text for JSON file from reddit rss\n",
    "\"\"\"    \n",
    "    \n",
    "    file_str = \"\"\n",
    "              \n",
    "    if (feed['bozo'] == 1):\n",
    "        print(\"Error Reading/Parsing Feed XML Data\")    \n",
    "    else:\n",
    "        for item in feed[ \"items\" ]:\n",
    "            dttm = item[ \"date\" ]\n",
    "            title = item[ \"title\" ]\n",
    "            summary_text = text_from_html(item[ \"summary\" ])\n",
    "            link = item[ \"link\" ]\n",
    "        \n",
    "            file_str += \"====================\\n\"\n",
    "            file_str += \"Title: {} ({})\\nTimestamp: {}\\n\".format(title,link,dttm)\n",
    "            file_str += \"--------------------\\nSummary:\\n{}\".format(summary_text)\n",
    "\n",
    "    return file_str\n",
    "\"\"\"\n",
    "#---------------------------------------Creating a file name------------------\n",
    "\n",
    "import time\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "print(timestr)\n",
    "\n",
    "file_name = \"reddit-unpop-rss-\" + timestr + \".txt\"\n",
    "## print(file_name)\n",
    "#reddit_thread_string = reddit_post_string(feed)\n",
    "\n",
    "# I actually should not need this code---------------------------------------------------------------\n",
    "# unpop_df = pd.DataFrame.from_dict(feed[\"items\"])\n",
    "# Instead we should just be able to upload the scraped feed as is which would save as JSON\n",
    "\n",
    "#upload_as_blob(my_bucket_name, reddit_thread_string, file_name)\n",
    "# THIS IS ANOTHER OLDER VERSION SAVING THE DATAFRAME -------------> upload_as_blob(my_bucket_name, unpop_df, file_name)\n",
    "upload_as_blob(my_bucket_name, feed, file_name)\n",
    "\n",
    "# list_blobs(my_bucket_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have data loaded to blob, we can continue with rest of project"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import feedparser\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    return u\" \".join(t.strip() for t in visible_texts)\n",
    "\n",
    "a_reddit_rss_url = 'http://www.reddit.com/r/unpopularopinion/.rss?sort=new'\n",
    "\n",
    "feed = feedparser.parse( a_reddit_rss_url )\n",
    "\n",
    "\"\"\"\n",
    "if (feed['bozo'] == 1):\n",
    "    print(\"Error Reading/Parsing Feed XML Data\")    \n",
    "else:\n",
    "    for item in feed[ \"items\" ]:\n",
    "        dttm = item[ \"date\" ]\n",
    "        title = item[ \"title\" ]\n",
    "        summary_text = text_from_html(item[ \"summary\" ])\n",
    "        link = item[ \"link\" ]\n",
    "        \n",
    "        print(\"====================\")\n",
    "        print(\"Title: {} ({})\\nTimestamp: {}\".format(title,link,dttm))\n",
    "        print(\"--------------------\\nSummary:\\n{}\".format(summary_text))\n",
    "\"\"\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to download the blob as a string and turn it in to a list of items for later use\n",
    "\n",
    "blob_list = []\n",
    "\n",
    "for blob_item in list_blobs(my_bucket_name):\n",
    "    blob_list.append(read_blob_as_string(my_bucket_name, blob_item))\n",
    "\n",
    "new_sum_list = []\n",
    "    \n",
    "for blob in blob_list:\n",
    "    for item in blob[\"items\"]:\n",
    "        new_sum_list.append(text_from_html(item[\"summary\"]))\n",
    "    \n",
    "    \n",
    "#---------------Saving below for reference-------------\n",
    "    \n",
    "sum_list = []\n",
    "\n",
    "for item in feed[\"items\"]:\n",
    "    sum_list.append(text_from_html(item[\"summary\"]))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#!pip install google-cloud-language\n",
    "#!pip install --upgrade protobuf\n",
    "!pip uninstall protobuf python3-protobuf\n",
    "!pip install --upgrade pip\n",
    "!pip install --upgrade protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Hello, world!\n",
      "Sentiment: 0.6000000238418579, 0.6000000238418579\n"
     ]
    }
   ],
   "source": [
    "# Imports the Google Cloud client library\n",
    "from google.cloud import language_v1\n",
    "from google.cloud import language\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# Key is saved in this folder and was uploaded to the cloud shell \n",
    "creds=service_account.Credentials.from_service_account_file('umc-dsa-8420-fs2021-82e06d434dd0.json')\n",
    "\n",
    "# Instantiates a client\n",
    "client = language_v1.LanguageServiceClient(credentials=creds,)\n",
    "\n",
    "\n",
    "# The text to analyze\n",
    "text = u\"Hello, world!\"\n",
    "document = language_v1.Document(\n",
    "    content=text, type_=language_v1.Document.Type.PLAIN_TEXT\n",
    ")\n",
    "\n",
    "# Detects the sentiment of the text\n",
    "sentiment = client.analyze_sentiment(\n",
    "    request={\"document\": document}\n",
    ").document_sentiment\n",
    "\n",
    "print(\"Text: {}\".format(text))\n",
    "print(\"Sentiment: {}, {}\".format(sentiment.score, sentiment.magnitude))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports the Google Cloud client library\n",
    "from google.cloud import language_v1\n",
    "from google.cloud import language\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# Key is saved in this folder and was uploaded to the cloud shell \n",
    "creds=service_account.Credentials.from_service_account_file('umc-dsa-8420-fs2021-82e06d434dd0.json')\n",
    "\n",
    "# Instantiates a client\n",
    "client = language_v1.LanguageServiceClient(credentials=creds,)\n",
    "\n",
    "def sentiment_dict(text_list):\n",
    "    \n",
    "    text_vector = []\n",
    "    sentiment_score = []\n",
    "    sentiment_mag = []\n",
    "    text_dict = {}\n",
    "    \n",
    "    for item in text_list:\n",
    "        document = language_v1.Document(\n",
    "            content=item, type_=language_v1.Document.Type.PLAIN_TEXT\n",
    "            )\n",
    "\n",
    "        # Detects the sentiment of the text\n",
    "        sentiment = client.analyze_sentiment(\n",
    "            request={\"document\": document}\n",
    "            ).document_sentiment\n",
    "\n",
    "        #print(\"Text: {}\".format(item))\n",
    "        #print(\"Sentiment: {}, {}\".format(sentiment.score, sentiment.magnitude))\n",
    "        \n",
    "        text_vector.append(item)\n",
    "        sentiment_score.append(sentiment.score)\n",
    "        sentiment_mag.append(sentiment.magnitude)\n",
    "    \n",
    "    text_dict['text'] = text_vector\n",
    "    text_dict['sentiment_score'] = sentiment_score\n",
    "    text_dict['sentiment_magnitude'] = sentiment_mag\n",
    "    \n",
    "    return text_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_dictionary = sentiment_dict(sum_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>sentiment_magnitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Greetings, you opinionated, unpopular lot! Thi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Going to be quick and concise about this, cudd...</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You paid over $1k to be a walking billboard fo...</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It's so annoying. I have a 1.5 year old labrad...</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>4.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What I meant by that , is testing their patien...</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment_score  \\\n",
       "0  Greetings, you opinionated, unpopular lot! Thi...              0.0   \n",
       "1  Going to be quick and concise about this, cudd...             -0.6   \n",
       "2  You paid over $1k to be a walking billboard fo...             -0.4   \n",
       "3  It's so annoying. I have a 1.5 year old labrad...             -0.5   \n",
       "4  What I meant by that , is testing their patien...             -0.4   \n",
       "\n",
       "   sentiment_magnitude  \n",
       "0                  1.6  \n",
       "1                  4.1  \n",
       "2                  3.6  \n",
       "3                  4.8  \n",
       "4                  1.4  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unpop_df = pd.DataFrame.from_dict(sentiment_dictionary)\n",
    "\n",
    "unpop_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f10900f5d68>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "unpop_df.plot.scatter('sentiment_magnitude', 'sentiment_score')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing your submission\n",
    "\n",
    "### Deliverables: \n",
    "   1. This or a replacement Notebook\n",
    "   1. An aggregateion of data in tabular format that conveyes something interesting about the Reddit RSS feed during your scraping.\n",
    "     * The table can be embedded or uploaded into this folder (CSV or Excel)\n",
    "   1. One or more data visualizations\n",
    "\n",
    "Imbed your image into this page by saving your data visualization as: `FINAL_PROJECT_IMAGE.png`  \n",
    "Upload it to the `module8/exercises/` folder.\n",
    "\n",
    "If you need to, change the file type to `.jpg` or `.jpeg` or ... whatever, then update the link in this cell (double click to edit).  \n",
    "Then re-run this markdown cell to see it.\n",
    "\n",
    "![FINAL_PROJECT_IMAGE.png MISSING](./exercises/FINAL_PROJECT_IMAGE.png)\n",
    "\n",
    "---\n",
    "## Summarize in the fields below\n",
    " 1. Describe the overall process and components you used for the project.\n",
    " 2. What is the key insight from the tabularization?\n",
    " 3. What is the key insight from the visualization?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### 1. \n",
    "# --------------------------\n",
    "\n",
    "I began by pulling the reddit data in through the RSS feed for the subreddit \"Unpopular Opinion\". The goal of this project is to understand the sentiment behind the posted unpopular opinions on the subreddit and answer questions such as \"Are the opinions always negative opinions, or are people's negative opinions posted in a positive light to earn favor?\".\n",
    "\n",
    "From there, I upload the rss feed as is into a txt file on the google cloud storage bin. This way we can continue to pull in new feeds and save them to the cloud.\n",
    "\n",
    "When processing the data, first we pull back in the specific text file we want to analyze and run it through the google NLP API in order to get the sentiment scores and sentiment magnitude. I put this code in a function that then creates a dictionary from the relevant information that will be easy to turn into a dataframe for later use. \n",
    "\n",
    "Once the function has run and we have the dictionary, we turn that into a dataframe to complete the 'tabular' requirement of this task. Once we have this tabular form we can do more with it like create a scatter plot such as the one above which shows the distribution of the sentiment scores compared to magnitude. Overall we can see that people typically post unpopular opinions using negative language/sentiment. \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### 2. \n",
    "# --------------------------\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### 3.\n",
    "# --------------------------\n",
    "\n",
    "For the visualization that I did, the main takeaway is a majority of the text sentiment is clustered below 0.0 indicated that a majority of the posts are indeed negative in sentiment. This is not a surprising find seeing as the subreddit is 'unpopular opinion'. What is interesting about this is it goes to show that not only are people asking if their opinions are popular or not, but the way the write about the topics typically are in a negative sense as though they may be talking negatively about the opinion they are against, rather than talking positively about the opinion they are expressing.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Save your Notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
