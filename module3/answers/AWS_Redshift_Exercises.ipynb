{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Redshift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to load the StackOverFlow developer survey 2017 data into reshift cluster and query the data. The function getpass(\"password\") will capture a password for the database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### SET THE FOLLOWING PARAMETERS ###################################################\n",
    "#Set the AWS Region\n",
    "region = 'us-west-2'\n",
    "\n",
    "#Set the AWS Access ID (Given to you buy the DSA staff)\n",
    "access_id = '<>' \n",
    "\n",
    "#Set the AWS Access Key (Given to you buy the DSA staff)\n",
    "access_key = '<>' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import psycopg2\n",
    "from getpass import getpass\n",
    "from pandas import read_sql\n",
    "import datetime\n",
    "\n",
    "redshift_client = boto3.client('redshift',region_name=region, \n",
    "                   aws_access_key_id = access_id, \n",
    "                   aws_secret_access_key = access_key)\n",
    "\n",
    "# Give a password to your redshift cluster\n",
    "pwd = '<>' # Choose a password. It must be at least 8 characters long, must contain at least 1 decimal digit, and \n",
    "         #must contain at least 1 upper case character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the names of the security group for the cluster, names of the cluster and database itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sec_group_name= \"climate_sec_group\"\n",
    "cluster_name=\"climate\"\n",
    "database_name=\"climatecitydata\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an AWS EC2 client object to create a security group for the redshift cluster. We are going to deploy the cluster in us-west-1 region. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec2_client = boto3.client('ec2',region_name=region, \n",
    "                   aws_access_key_id = access_id, \n",
    "                   aws_secret_access_key = access_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Security group named \"redshift_Sec_group\" is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg = ec2_client.create_security_group(\n",
    "    Description='security group for redhift cluster',\n",
    "    GroupName=Sec_group_name\n",
    ")\n",
    "Sec_group=sg[\"GroupId\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit the security group inbound rules to allow all TCP/IP traffic on port number 5439. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sec_rule=\"ALL TCP\"\n",
    "    data = ec2_client.authorize_security_group_ingress(\n",
    "        GroupId=Sec_group,\n",
    "        IpPermissions=[\n",
    "            {'IpProtocol': 'tcp',\n",
    "             'FromPort': 5439,\n",
    "             \n",
    "             'ToPort': 5439,\n",
    "             'IpRanges': [{'CidrIp': '0.0.0.0/0'}]},\n",
    "        ],)\n",
    "    print(\"Ingress \"+sec_rule+\" added\")\n",
    "except:\n",
    "    print(sec_rule+\" already added\")\n",
    "#     print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a keypair\n",
    "\n",
    "Create a keypair for the EC2 instance. We first generate a name to create a key with that name and also store the key in a file. ec2.create_key_pair() will create a keypair. System command echo is used to write the contents of keypair generated to a file created with same name as keypair. \n",
    "\n",
    "You have to modify the file permissions to provide readonly access. If the file is open, system will throw an error. Do chmod(file, 0o400) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import os\n",
    "import getpass\n",
    "from subprocess import call\n",
    "\n",
    "#Set the username from system\n",
    "system_user_name=getpass.getuser()\n",
    "\n",
    "ec2_pem_file=time.strftime(\"EC2-%d%m%Y%H%M%S-\"+system_user_name)\n",
    "ec2_key=ec2_client.create_key_pair(KeyName=ec2_pem_file)\n",
    "\n",
    "#Don't do this unless you have a good reason\n",
    "#print(emr_key['KeyMaterial'])\n",
    "\n",
    "os.system(\"echo \\\"\"+ec2_key['KeyMaterial']+\"\\\" > \"+ec2_pem_file+\".pem\")\n",
    "os.chmod(ec2_pem_file+\".pem\",0o400)\n",
    "\n",
    "print(\"KeyName         : \"+ec2_key['KeyName']+\"\\nKey Fingerprint : \"+ec2_key['KeyFingerprint'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below cell will deploy a redshift cluster. A default database named \"sof_survey\" is created during the cluster is deployed. The parameter \"NumberOfNodes\" will tell how many slave nodes the cluster should have. The security group created above is used. At the end of the session we will delete the security group. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = redshift_client.create_cluster(\n",
    "    DBName=database_name,            # Optional. A default database named dev is created for the cluster. Optionally, \n",
    "                                     # specify a custom database name (e.g. mydb) to create an additional database.\n",
    "    \n",
    "    ClusterIdentifier=cluster_name,  # Unique key that identifies a cluster. It is stored as a lowercase string. \n",
    "    ClusterType='multi-node',        # single-node is other option\n",
    "    NodeType='dc1.large',            # other options are dc1.8xlarge ds2.xlarge ds2.8xlarge ds1.xlarge ds1.8xlarge\n",
    "    MasterUsername='your_pawprint',  # Add your pawprint\n",
    "    MasterUserPassword=pwd,\n",
    "#     ClusterSubnetGroupName='default',\n",
    "    VpcSecurityGroupIds=[\n",
    "        Sec_group,\n",
    "    ],\n",
    "    ClusterParameterGroupName='default.redshift-1.0',  # Parameter group to associate with this cluster.  \n",
    "    Port=5439,\n",
    "    AllowVersionUpgrade=True,\n",
    "    NumberOfNodes=2,   # Compute nodes store your data and execute your queries. In addition to your compute nodes, a leader \n",
    "                       # node will be added to your cluster, free of charge. The leader node is the access point for \n",
    "                       # ODBC/JDBC and generates the query plans executed on the compute nodes.\n",
    "    \n",
    "    PubliclyAccessible=True, # If true, cluster to be accessible from the public internet. If No, then its accessible only \n",
    "                             # from within the private VPC network\n",
    "    EnhancedVpcRouting=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Below poll function keeps checking the status of cluster. Once it is in reday state the poll function breaks out of the loop indicating the cluster is available for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poll_until_completed(client, cluster_id):\n",
    "    delay = 2\n",
    "    while True:\n",
    "        cluster = client.describe_clusters(ClusterIdentifier=cluster_id)\n",
    "#         for tag in cluster:\n",
    "#             print(tag)\n",
    "#         print(cluster)\n",
    "#         print(cluster['Clusters'][0]['ClusterIdentifier'])\n",
    "        status = cluster['Clusters'][0]['ClusterStatus']\n",
    "#         message = cluster.get('Message', '')\n",
    "        now = str(datetime.datetime.now().time())\n",
    "        print(\"cluster %s is %s at %s\" % (cluster_id, status, now))\n",
    "        if status in ['available', 'final-snapshot']:\n",
    "            break\n",
    "\n",
    "        # exponential backoff with jitter\n",
    "        delay *= random.uniform(1.1, 2.0)\n",
    "        time.sleep(delay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poll_until_completed(redshift_client, cluster_id=cluster_name)  # Can't use the cluster until it is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the cell if you want to see the complete details of cluster. \n",
    "\n",
    "# redshift_client.describe_clusters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To connect to the cluster we need its endpoint. Below cell prints the end point, the default port where the cluster is listening for input requests and the database available in the cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_end_point = ''\n",
    "for cluster in redshift_client.describe_clusters()[\"Clusters\"]:\n",
    "    print(\"Cluster endpoint:\",str(cluster[\"Endpoint\"][\"Address\"])+\"\\n\"+\"Port:\",str(cluster[\"Endpoint\"][\"Port\"])+\"\\n\"+\"Database:\",str(cluster[\"DBName\"]))\n",
    "    cluster_end_point = str(cluster[\"Endpoint\"][\"Address\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code cell prints the public and private addresses of the nodes in cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in redshift_client.describe_clusters()[\"Clusters\"]:\n",
    "    for ClusterNode in cluster[\"ClusterNodes\"]:\n",
    "        if cluster_name in cluster[\"Endpoint\"][\"Address\"]:\n",
    "            print(ClusterNode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connection string below is used to connect to \"sof_survey\" database in \"stackoverflow\" using port 5439.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_string = { 'dbname': database_name, \n",
    "           'user':'your_pawprint', #Add your pawprint\n",
    "           'pwd':pwd,\n",
    "           'host':cluster_end_point,\n",
    "           'port':'5439'\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conn(config):\n",
    "    try:\n",
    "        con=psycopg2.connect(dbname=config['dbname'], host=config['host'], \n",
    "                              port=config['port'], user=config['user'], \n",
    "                              password=config['pwd'])\n",
    "        return con\n",
    "    except Exception as err:\n",
    "        print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = create_conn(config=conn_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have established the connection to redshift cluster using psycopg library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the data to S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3 = boto3.resource('s3',aws_access_key_id = access_id, \n",
    "                   aws_secret_access_key = access_key)\n",
    "\n",
    "\n",
    "bucket_name=time.strftime(system_user_name+\"bucket%S\")\n",
    "\n",
    "s3.create_bucket(Bucket=bucket_name)\n",
    "\n",
    "s3.Object(bucket_name, 'GlobalLandTemperaturesByCity.csv').put(Body=open('GlobalLandTemperaturesByCity.csv', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = s3.Bucket(bucket_name)\n",
    "bucket.Acl().put(ACL='public-read')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, ast, psycopg2\n",
    "\n",
    "import csv\n",
    "f = open('GlobalLandTemperaturesByCity.csv', 'r', encoding='latin-1')\n",
    "reader = csv.reader(f)\n",
    "    \n",
    "# f = open('globalterrorismdb.csv', 'r')\n",
    "# reader = csv.reader(f)\n",
    "print(type(reader))\n",
    "# Below line of code, will assign empty lists to variables longest, headers and type_list. We will use these variables in cells \n",
    "# Below when determining the type and size of each column in the table. \n",
    "# longest: holds the column size \n",
    "# headers: holds the column headers\n",
    "# type_list: holds the column types in the dataset\n",
    "\n",
    "longest, headers, type_list = [], [], []\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "# df = pd.read_csv(\"globalterrorismdb.csv\",encoding='latin-1',low_memory=False)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataType(val, current_type):\n",
    "    try:\n",
    "        # Evaluates numbers to an appropriate type, and strings an error\n",
    "        t = ast.literal_eval(val)\n",
    "    except ValueError:\n",
    "        return 'varchar'\n",
    "    except SyntaxError:\n",
    "        return 'varchar'\n",
    "    \n",
    "    if type(t) in [int, float]:\n",
    "        if (type(t) in [int]) and current_type not in ['float', 'varchar']:\n",
    "           # Use smallest possible int type\n",
    "            if (-32768 < t < 32767) and current_type not in ['int', 'bigint']:\n",
    "                return 'smallint'\n",
    "            elif (-2147483648 < t < 2147483647) and current_type not in ['bigint']:\n",
    "                return 'int'\n",
    "            else:\n",
    "                return 'bigint'\n",
    "        if type(t) is float and current_type not in ['varchar']:\n",
    "            return 'decimal'\n",
    "    elif type(t) in [datetime]:\n",
    "        return 'date'\n",
    "    else:\n",
    "        return 'varchar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def dataType(val):\n",
    "#     try:\n",
    "#         # Evaluates numbers to an appropriate type, and strings an error\n",
    "#         t = ast.literal_eval(val)\n",
    "#     except ValueError:\n",
    "#         return 'varchar'\n",
    "#     except SyntaxError:\n",
    "#         return 'varchar'\n",
    "    \n",
    "\n",
    "# # check if the cell value is integer type. If yes, return integer. In else case, check if its float type. Return 'float' if yes \n",
    "# # or return 'varchar' as the data type of cell.\n",
    "#     try:\n",
    "#         if isinstance(t, int):\n",
    "#             return \"int\"\n",
    "#         elif isinstance(t, float):\n",
    "#             return \"float\"\n",
    "#         else:\n",
    "#             return \"varchar\"\n",
    "#     except Exception as e:\n",
    "#         print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in reader:\n",
    "    if len(headers) == 0:\n",
    "        headers = row\n",
    "        for col in row:\n",
    "            longest.append(0)\n",
    "            type_list.append('')\n",
    "    else:\n",
    "        for i in range(len(row)):\n",
    "            # NA is the csv null value\n",
    "            if row[i] == 'NA':\n",
    "                pass\n",
    "            else:\n",
    "                var_type = dataType(row[i], type_list[i])\n",
    "                type_list[i] = var_type\n",
    "                if len(row[i]) > longest[i]:\n",
    "                    longest[i] = len(row[i])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for row in reader:\n",
    "#     if len(headers) == 0:\n",
    "#         headers = row\n",
    "#         for col in row:\n",
    "#             longest.append(0)\n",
    "#             type_list.append('')\n",
    "            \n",
    "            \n",
    "#     else:\n",
    "# #         print(type_list)\n",
    "# #         print(len(row))\n",
    "#         for i in range(len(row)):\n",
    "#             if row[i] == 'NA':\n",
    "#                 pass\n",
    "#             else:\n",
    "#                 type_list[i] = dataType(row[i])\n",
    "#                 if len(row[i]) > longest[i]:\n",
    "#                     longest[i] = len(row[i])+5       \n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement = 'create table '+database_name+' ('\n",
    "\n",
    "for i in range(len(headers)):\n",
    "    if type_list[i] == 'varchar':\n",
    "        statement = (statement + '{} varchar({}),').format(headers[i].lower(), str(longest[i]))\n",
    "    else:\n",
    "        statement = (statement + '{} {}' + ',').format(headers[i].lower(), type_list[i])\n",
    "\n",
    "statement = statement[:-1] + ');'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement='create table dsaclimatecitydata (dt date,averagetemperature numeric(10,5),averagetemperatureuncertainty numeric(10,5),city varchar(25),country varchar(34),latitude varchar(6),longitude varchar(7));'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = read_sql(\"delete global_ter_data;\",con=con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = con.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(statement)\n",
    "con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# con.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name=\"rsgt3bbucket05\"\n",
    " s3 = boto3.resource('s3',region_name=region, \n",
    "                   aws_access_key_id = access_id, \n",
    "                   aws_secret_access_key = access_key)\n",
    "\n",
    "bucket = s3.Bucket(bucket_name)\n",
    "bucket.Acl().put(ACL='public-read')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"s3://\"+bucket_name+\"/GlobalLandTemperaturesByCity.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"copy dsaclimatecitydata from 's3://\"\"\"+bucket_name+\"\"\"/GlobalLandTemperaturesByCity.csv'\n",
    "    access_key_id 'your_access_key_id' \n",
    "    secret_access_key 'your_secret_access_key'\n",
    "    region 'us-east-1'\n",
    "    ignoreheader 1\n",
    "    null as 'NA'\n",
    "    removequotes\n",
    "    delimiter ',';\"\"\"\n",
    "cur.execute(sql)\n",
    "con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=read_sql(\"select *from stl_load_errors\",con)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_sql(\"select * from dsaclimatecitydata limit 10;\",con=con)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a table and load the data into Redshift. We established connection to the cluster above. Use the connection object \"con\" to execute create table srtatement.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_sql(\"select column_name, data_type, character_maximum_length \\\n",
    "from INFORMATION_SCHEMA.COLUMNS where table_name = 'dsaclimatecitydata';\",con=con)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will stage the data on S3 first before writing it to redshift cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_sql(\"\"\"select * from dsaclimatecitydata where city = 'Hyderabad' limit 5;\"\"\",con)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement = \"\"\"create table dsaclimatesubsetdata1 SORTKEY (dt) as select dt, averagetemperature, DATE_PART(month,dt) as month, \n",
    "                DATE_PART(year,dt) as year, city, country, latitude, longitude from dsaclimatecitydata;\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(statement)\n",
    "con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_sql(\"\"\"delete new_table\"\"\",con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_sql(\"\"\"select * from dsaclimatesubsetdata1 limit 5;\"\"\",con)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_sql(\"select column_name, data_type, character_maximum_length \\\n",
    "from INFORMATION_SCHEMA.COLUMNS where table_name = 'dsaclimatesubsetdata1';\",con=con)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_sql(\"\"\"select coalesce(median(averagetemperature),0) as median, year from dsaclimatesubsetdata1 \n",
    "                group by year order by year;\"\"\",con)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the data\n",
    "plt.plot(df['year'],df['median'])\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_sql(\"\"\"select coalesce(median(averagetemperature),0) as median, year from dsaclimatesubsetdata1 \n",
    "                 group by year having year >=1900 order by year;\"\"\",con)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the data\n",
    "plt.plot(df['year'],df['median'])\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "\n",
    "\n",
    "trace=go.Scatter(\n",
    "    x=df['year'],\n",
    "    y=df['median'],\n",
    "    mode='lines',\n",
    "    )\n",
    "data=[trace]\n",
    "\n",
    "py.iplot(data, filename='line-mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# li=['United States','China','India','Japan','Germany','United Kingdom']\n",
    "\n",
    "df = read_sql(\"\"\"select coalesce(median(averagetemperature),0) as median, year, country from dsaclimatesubsetdata1\n",
    "                 where country in ('United States','China','India','Japan','Germany','United Kingdom') \n",
    "                 group by year,country having year >=1950 order by year;\"\"\",con)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc=df.pivot('year','country','median')\n",
    "f,ax=plt.subplots(figsize=(20,10))\n",
    "abc.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_sql(\"\"\"select coalesce(median(averagetemperature),0) as median, year, country from dsaclimatesubsetdata1\n",
    "                 where country in ('United States','China','India','Japan','Germany','United Kingdom') \n",
    "                 group by year,country order by year DESC limit 10;\"\"\",con)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_sql(\"\"\"select coalesce(max(averagetemperature),0) as max_temp, \n",
    "              country from dsaclimatesubsetdata1 group by country order by max_temp desc limit 20;\"\"\",con)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "# sns.despine(left=True, bottom=True)\n",
    "bar_plot = sns.barplot(x=df[\"max_temp\"], y=df[\"country\"], palette=\"muted\",orient=\"h\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_sql(\"\"\"select coalesce(max(averagetemperature),0) as max_temp, \n",
    "              city from dsaclimatesubsetdata1 group by city order by max_temp desc limit 20;\"\"\",con)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "# sns.despine(left=True, bottom=True)\n",
    "bar_plot = sns.barplot(x=df[\"max_temp\"], y=df[\"city\"], palette=\"muted\",orient=\"h\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redshift_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = redshift_client.delete_cluster(\n",
    "#     ClusterIdentifier='climate',\n",
    "#     SkipFinalClusterSnapshot=True\n",
    "# )\n",
    "\n",
    "response = redshift_client.delete_cluster(\n",
    "    ClusterIdentifier=cluster_name,\n",
    "    SkipFinalClusterSnapshot=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
