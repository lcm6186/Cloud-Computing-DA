{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS EMR Launcher\n",
    "Welcome to the DSA Hadoop Launcher. Here you will add your key and a couple paramaters to deploy your own Hadoop stack on AWS.\n",
    "\n",
    "Please set the paramaters in the \"SET THE FOLLOWING PARAMETERS\" box below and run the first cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch an EMR cluster with a Jupyter Notebook\n",
    "\n",
    "################################### SET THE FOLLOWING PARAMETERS ###################################################\n",
    "#Set the AWS Region\n",
    "region = 'us-west-2'\n",
    "\n",
    "#Set the AWS Access ID (Given to you buy the DSA staff)\n",
    "access_id = '_____________'\n",
    "\n",
    "#Set the AWS Access Key (Given to you buy the DSA staff)\n",
    "access_key = '__________________________'\n",
    "\n",
    "#Set the Size of the AWS EC2 Instances\n",
    "instance_size = 'm3.xlarge'\n",
    "\n",
    "#Set the Number of Master Instances\n",
    "master_instances = 1\n",
    "\n",
    "#Set the Number of Slave Instances\n",
    "slave_instances = 2\n",
    "\n",
    "#Dataset Provided By Your Instructor\n",
    "dataset_location = 'https://s3-us-west-2.amazonaws.com/dataset-store/amazon_reviews/reviews.json'\n",
    "dataset_file_name = 'reviews.json'\n",
    "\n",
    "#Folder Name for Notebooks Transferring to AWS\n",
    "load_notebook_location = 'notebooks'\n",
    "####################################################################################################################\n",
    "\n",
    "\n",
    "#Import AWS Tools\n",
    "import boto3\n",
    "\n",
    "#Establish The EMR Session\n",
    "emr = boto3.client(\n",
    "   'emr',\n",
    "    region_name=region, \n",
    "    aws_access_key_id = access_id, \n",
    "    aws_secret_access_key = access_key\n",
    ")\n",
    "\n",
    "#Establish The EC2 Session\n",
    "ec2 = boto3.client(\n",
    "    'ec2',\n",
    "    region_name=region, \n",
    "    aws_access_key_id = access_id, \n",
    "    aws_secret_access_key = access_key\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\".\") # add current folder for search path to find fabric\n",
    "\n",
    "#Import System Tools\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import getpass\n",
    "from subprocess import call\n",
    "import fabric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Set important Variables\n",
    "system_user_name=getpass.getuser()\n",
    "wk_dir=os.getcwd()\n",
    "\n",
    "print(emr)\n",
    "print(ec2)\n",
    "\n",
    "#Price Calculator Development In Progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create SSH Keypair\n",
    "This will create a temporary keypair for you to access your cluster and save it to your current working directory. \n",
    "\n",
    "This is automatic so please run this cell as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SSH Keypair File For This EMR Cluster\n",
    "\n",
    "emr_pem_file=time.strftime(\"EMR-%d%m%Y%H%M%S-\"+system_user_name)\n",
    "emr_key=ec2.create_key_pair(KeyName=emr_pem_file)\n",
    "\n",
    "#Don't do this unless you have a good reason\n",
    "#print(emr_key['KeyMaterial'])\n",
    "\n",
    "os.system(\"echo \\\"\"+emr_key['KeyMaterial']+\"\\\" > \"+emr_pem_file+\".pem\")\n",
    "os.chmod(wk_dir+\"/\"+emr_pem_file+\".pem\",0o400)\n",
    "\n",
    "print(\"KeyName         : \"+emr_key['KeyName']+\"\\nKey Fingerprint : \"+emr_key['KeyFingerprint'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch EMR Cluster\n",
    "This step will launch your Hadoop cluster. From this point on you will be charged money for every hour that this cluster is running. Please proceed with caution.\n",
    "\n",
    "All arguments for the following cell have been set in the first cell. Please run the following cell as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wait for Bootstrap and Print Cluster Details\n",
    "print (\"\\n***Please Wait***\\n\\n\"+response['Cluster']['Status']['State']+\".\",end=\"\")\n",
    "while True:\n",
    "    response = emr.describe_cluster(\n",
    "        ClusterId=cluster_id  \n",
    "    )\n",
    "    try:\n",
    "        response['Cluster']['MasterPublicDnsName'].find(\"ec2\")\n",
    "        print('...Cluster DNS Active',end=\"\")\n",
    "        break\n",
    "    except:    \n",
    "        time.sleep(5)\n",
    "        print(\".\", end=\"\")\n",
    "        pass\n",
    "\n",
    "print(\"\\n\\nProceeding with Firewall Rules...\")\n",
    "\n",
    "#Get Cluster Security Group Info\n",
    "master_security_group = response['Cluster']['Ec2InstanceAttributes']['EmrManagedMasterSecurityGroup']\n",
    "slave_security_group = response['Cluster']['Ec2InstanceAttributes']['EmrManagedSlaveSecurityGroup']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch an EMR cluster\n",
    "\n",
    "response = emr.run_job_flow(\n",
    "   Name='EMR Jupyter NB-'+system_user_name,\n",
    "   LogUri='s3n://logs-'+system_user_name+'/elasticmapreduce/',\n",
    "   ReleaseLabel='emr-5.28.0',\n",
    "   Instances={\n",
    "       \n",
    "       'InstanceGroups': [\n",
    "           {\n",
    "               'Name':'Master - 1',\n",
    "               'InstanceRole':'MASTER',\n",
    "               'InstanceType': instance_size,\n",
    "               'InstanceCount': master_instances\n",
    "           },\n",
    "           {\n",
    "               'Name':'Core - 2',\n",
    "               'InstanceRole':'CORE',\n",
    "               'InstanceType': instance_size,\n",
    "               'InstanceCount': slave_instances\n",
    "           }\n",
    "       ],\n",
    "       'KeepJobFlowAliveWhenNoSteps': True,\n",
    "       'TerminationProtected':True,\n",
    "       'Ec2KeyName': emr_pem_file,\n",
    "       'Placement': {\n",
    "           'AvailabilityZone': 'us-west-2c'\n",
    "       }\n",
    "   },\n",
    "\n",
    "\n",
    "#Insert Steps Here if Applicable \n",
    "\n",
    "#Insert Bootstrapping Actions Here if Applicable\n",
    "\n",
    "   \n",
    "   AutoScalingRole=\"EMR_AutoScaling_DefaultRole\",\n",
    "   Applications=[\n",
    "       {\n",
    "           'Name': 'Hadoop'\n",
    "       },\n",
    "       {\n",
    "           'Name': 'Hive'\n",
    "       },\n",
    "       {\n",
    "           'Name': 'Spark'\n",
    "       },\n",
    "       {\n",
    "           'Name': 'Pig'\n",
    "       }\n",
    "   ],\n",
    "   Configurations=[\n",
    "       {\n",
    "           'Classification': 'spark',\n",
    "           'Configurations': [],\n",
    "           'Properties': {\n",
    "               'maximizeResourceAllocation':'true'\n",
    "           }\n",
    "       },\n",
    "   ],\n",
    "   VisibleToAllUsers=False,\n",
    "   EbsRootVolumeSize=10,\n",
    "   JobFlowRole='EMR_EC2_DefaultRole',\n",
    "   ServiceRole='EMR_DefaultRole',\n",
    "   #ScaleDownBehavior='TERMINATE_AT_INSTANCE_HOUR', #For reliese 5.0.0+\n",
    "    \n",
    ")#End of Cluster Launch Command\n",
    "\n",
    "#Define Cluster ID\n",
    "cluster_id = response['JobFlowId']\n",
    "#Get Cluster Info\n",
    "response = emr.describe_cluster(\n",
    "    ClusterId=cluster_id  \n",
    ")\n",
    "print (\"Cluster Name : \"+response['Cluster']['Name']+\"\\nCluster ID   : \"+response['Cluster']['Id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Firewall Exceptions\n",
    "try:\n",
    "    sec_rule=\"SSH\"\n",
    "    data = ec2.authorize_security_group_ingress(\n",
    "        GroupId=master_security_group,\n",
    "        IpPermissions=[\n",
    "            {'IpProtocol': 'tcp',\n",
    "             'FromPort': 22,\n",
    "             'ToPort': 22,\n",
    "             'IpRanges': [{'CidrIp': '128.206.0.0/16'}]},\n",
    "        ])\n",
    "    print(\"Ingress \"+sec_rule+\" added\")\n",
    "except:\n",
    "    print(sec_rule+\" already added\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sec_rule=\"YARN\"\n",
    "    data = ec2.authorize_security_group_ingress(\n",
    "        GroupId=master_security_group,\n",
    "        IpPermissions=[\n",
    "            {'IpProtocol': 'tcp',\n",
    "             'FromPort': 8088,\n",
    "             'ToPort': 8088,\n",
    "             'IpRanges': [{'CidrIp': '128.206.0.0/16'}]},\n",
    "        ])\n",
    "    print(\"Ingress \"+sec_rule+\" added\")\n",
    "except:\n",
    "    print(sec_rule+\" already added\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sec_rule=\"HDFS NameNode\"\n",
    "    data = ec2.authorize_security_group_ingress(\n",
    "        GroupId=master_security_group,\n",
    "        IpPermissions=[\n",
    "            {'IpProtocol': 'tcp',\n",
    "             'FromPort': 50070,\n",
    "             'ToPort': 50070,\n",
    "             'IpRanges': [{'CidrIp': '128.206.0.0/16'}]},\n",
    "        ])\n",
    "    print(\"Ingress \"+sec_rule+\" added\")\n",
    "except:\n",
    "    print(sec_rule+\" already added\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sec_rule=\"Spark History Server\"\n",
    "    data = ec2.authorize_security_group_ingress(\n",
    "        GroupId=master_security_group,\n",
    "        IpPermissions=[\n",
    "            {'IpProtocol': 'tcp',\n",
    "             'FromPort': 18080,\n",
    "             'ToPort': 18080,\n",
    "             'IpRanges': [{'CidrIp': '128.206.0.0/16'}]},\n",
    "        ])\n",
    "    print(\"Ingress \"+sec_rule+\" added\")\n",
    "except:\n",
    "    print(sec_rule+\" already added\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sec_rule=\"Hue\"\n",
    "    data = ec2.authorize_security_group_ingress(\n",
    "        GroupId=master_security_group,\n",
    "        IpPermissions=[\n",
    "            {'IpProtocol': 'tcp',\n",
    "             'FromPort': 8888,\n",
    "             'ToPort': 8888,\n",
    "             'IpRanges': [{'CidrIp': '128.206.0.0/16'}]},\n",
    "        ])\n",
    "    print(\"Ingress \"+sec_rule+\" added\")\n",
    "except:\n",
    "    print(sec_rule+\" already added\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sec_rule=\"HBase\"\n",
    "    data = ec2.authorize_security_group_ingress(\n",
    "        GroupId=master_security_group,\n",
    "        IpPermissions=[\n",
    "            {'IpProtocol': 'tcp',\n",
    "             'FromPort': 16010,\n",
    "             'ToPort': 16010,\n",
    "             'IpRanges': [{'CidrIp': '128.206.0.0/16'}]},\n",
    "        ])\n",
    "    print(\"Ingress \"+sec_rule+\" added\")\n",
    "except:\n",
    "    print(sec_rule+\" already added\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sec_rule=\"Jupyter Notebook\"\n",
    "    data = ec2.authorize_security_group_ingress(\n",
    "        GroupId=master_security_group,\n",
    "        IpPermissions=[\n",
    "            {'IpProtocol': 'tcp',\n",
    "             'FromPort': 9090,\n",
    "             'ToPort': 9090,\n",
    "             'IpRanges': [{'CidrIp': '128.206.0.0/16'}]},\n",
    "        ])\n",
    "    print(\"Ingress \"+sec_rule+\" added\")\n",
    "except:\n",
    "    print(sec_rule+\" already added\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sec_rule=\"Slave SSH\"\n",
    "    data = ec2.authorize_security_group_ingress(\n",
    "        GroupId=slave_security_group,\n",
    "        IpPermissions=[\n",
    "            {'IpProtocol': 'tcp',\n",
    "             'FromPort': 22,\n",
    "             'ToPort': 22,\n",
    "             'IpRanges': [{'CidrIp': '128.206.0.0/16'}]},\n",
    "        ])\n",
    "    print(\"Ingress \"+sec_rule+\" added\")\n",
    "except:\n",
    "    print(sec_rule+\" already added\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sec_rule=\"Slave YARN NodeManager\"\n",
    "    data = ec2.authorize_security_group_ingress(\n",
    "        GroupId=slave_security_group,\n",
    "        IpPermissions=[\n",
    "            {'IpProtocol': 'tcp',\n",
    "             'FromPort': 8042,\n",
    "             'ToPort': 8042,\n",
    "             'IpRanges': [{'CidrIp': '128.206.0.0/16'}]},\n",
    "        ])\n",
    "    print(\"Ingress \"+sec_rule+\" added\")\n",
    "except:\n",
    "    print(sec_rule+\" already added\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sec_rule=\"Slave HDFS DataNode\"\n",
    "    data = ec2.authorize_security_group_ingress(\n",
    "        GroupId=slave_security_group,\n",
    "        IpPermissions=[\n",
    "            {'IpProtocol': 'tcp',\n",
    "             'FromPort': 50075,\n",
    "             'ToPort': 50075,\n",
    "             'IpRanges': [{'CidrIp': '128.206.0.0/16'}]},\n",
    "        ])\n",
    "    print(\"Ingress \"+sec_rule+\" added\")\n",
    "except:\n",
    "    print(sec_rule+\" already added\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"\\n\\nFinishing Startup.\\nThis will take a few minutes...\\n\\n***Please Wait***\\n\\nStarting.\",end=\"\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "while str(response['Cluster']['Status']['State']) == 'STARTING':\n",
    "        time.sleep(5)\n",
    "        print(\".\", end=\"\")\n",
    "        response = emr.describe_cluster(\n",
    "            ClusterId=cluster_id  \n",
    "        )\n",
    "print('...Done',end=\"\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"\\n\\nRunning Bootstrap Actions.\\nThis will take a few minutes...\\n\\n***Please Wait***\\n\\nBootstrapping.\",end=\"\")\n",
    "\n",
    "while str(response['Cluster']['Status']['State']) == 'BOOTSTRAPPING':\n",
    "        time.sleep(5)\n",
    "        print(\".\", end=\"\")\n",
    "        response = emr.describe_cluster(\n",
    "            ClusterId=cluster_id  \n",
    "        )\n",
    "print('...Done',end=\"\")  \n",
    "print('\\n\\nCluster Status: '+response['Cluster']['Status']['State'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paramiko\n",
    "help(paramiko.config.SSHConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Refresh Cluster Description\n",
    "response = emr.describe_cluster(\n",
    "    ClusterId=cluster_id  \n",
    ")\n",
    "\n",
    "host_string = response['Cluster']['MasterPublicDnsName']\n",
    "print(host_string)\n",
    "\n",
    "#Bootstrap Cluster with Fabric\n",
    "from fabric import tasks\n",
    "from fabric import Connection\n",
    "\n",
    "# env.host_string = response['Cluster']['MasterPublicDnsName']\n",
    "# env.user = 'hadoop'\n",
    "# env.key_filename = wk_dir+\"/\"+emr_pem_file+\".pem\"\n",
    "# env.warn_only\n",
    "# env.FABRIC_RUN_HIDE=\"true\"\n",
    "\n",
    "\n",
    "c = Connection(\n",
    "    host=host_string,\n",
    "    user=\"hadoop\",\n",
    "    connect_kwargs={\n",
    "        \"key_filename\": wk_dir+\"/\"+emr_pem_file+\".pem\",\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def install_jupyter(fab_conn):\n",
    "    fab_conn.run('sudo -u root pip install jupyter')\n",
    "    fab_conn.run('sudo -u root pip install toree')\n",
    "    fab_conn.run('export SPARK_HOME=/usr/lib/spark;export PYTHONPATH=$PYTHONPATH:$SPARK_HOME/python:$SPARK_HOME/python/lib')\n",
    "    fab_conn.run('sudo -u root /usr/local/bin/jupyter toree install --replace --spark_home=/usr/lib/spark --spark_opts=\"--master=local[*]\" --interpreters=Scala,PySpark,SparkR,SQL')\n",
    "    fab_conn.run('mkdir -p /home/hadoop/.jupyter/')\n",
    "    fab_conn.run('curl -o /home/hadoop/.jupyter/jupyter_notebook_config.py https://s3-us-west-2.amazonaws.com/dsa-mizzou/scripts/jupyter_notebook_config.py')\n",
    "    fab_conn.run('sudo -u root yum -y install tmux')\n",
    "    fab_conn.run('tmux new-session -d \"jupyter notebook --no-browser --config /home/hadoop/.jupyter/jupyter_notebook_config.py\"')\n",
    "\n",
    "def load_dataset(fab_conn):\n",
    "    fab_conn.run('/usr/bin/hadoop fs -mkdir Datasets')\n",
    "    fab_conn.run('curl '+dataset_location+' | hadoop fs -appendToFile - Datasets/'+dataset_file_name)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "print('\\nInstalling Jupyter...')        \n",
    "install_jupyter(c)\n",
    "print('\\nDone\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading Dataset...')\n",
    "load_dataset(c)\n",
    "print('\\nDone\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Upload Notebook Directory\n",
    "os.system(\"scp -o StrictHostKeyChecking=no -r -i \"+wk_dir+\"/\"+emr_pem_file+\".pem \"+wk_dir+\"/\"+load_notebook_location+\"/.\"+\" hadoop@\"+response['Cluster']['MasterPublicDnsName']+\":/home/hadoop/\" )\n",
    "\n",
    "print('Please Proceed to the Next Step')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Access your EMR Cluster's Interfaces\n",
    "\n",
    "#### For Web Interfaces Run the Following Cell\n",
    "\n",
    "\n",
    "We are interested in running the Jupyter notebook on cluster. So click on the first link for launching Jupyterhub on EMR cluster. Here you can find the notebooks you uploaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Web Addresses to EMR\n",
    "print(\"Jupyter Notebooks\\nhttp://\"+response['Cluster']['MasterPublicDnsName']+\":9090/\\n\")\n",
    "print(\"YARN ResourceManager\\nhttp://\"+response['Cluster']['MasterPublicDnsName']+\":8088/\\n\")\n",
    "print(\"Hadoop HDFS NameNode\\nhttp://\"+response['Cluster']['MasterPublicDnsName']+\":50070/\\n\")\n",
    "print(\"Spark HistoryServer\\nhttp://\"+response['Cluster']['MasterPublicDnsName']+\":18080/\\n\")\n",
    "print(\"Hue\\nhttp://\"+response['Cluster']['MasterPublicDnsName']+\":8888/\\n\")\n",
    "print(\"Ganglia\\nhttp://\"+response['Cluster']['MasterPublicDnsName']+\"/ganglia/\\n\")\n",
    "print(\"HBase UI\\nhttp://\"+response['Cluster']['MasterPublicDnsName']+\":16010/\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not doing anything in the terminal. So you dont have to worry about doing SSH into the master. Ignore below cell.\n",
    "\n",
    "\n",
    "#### For SSH, Run the Following Cell and See Instructions Below\n",
    " 1. Run the Cell below\n",
    " 1. Highlight the ssh line and press Ctrl+C to copy it to your local clipboard\n",
    " 1. Click the link below to open a termainal\n",
    " 1. Paste the SSH link in (Ctrl + V) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SSH to EMR\n",
    "print(\"ssh -i \"+wk_dir+\"/\"+emr_pem_file+\".pem\"+\" hadoop@\"+response['Cluster']['MasterPublicDnsName'])\n",
    "print(\"https://europa.dsa.missouri.edu/user/\"+system_user_name+\"/terminals/1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your pasted command and output should look similar to:\n",
    "\n",
    "```\n",
    "$ ssh -i /dsa/home/scottgs/jupyter/CloudComputingDataAnalytics/module5/labs/EMR-12022019234354-scottgs.pem hadoop@ec2-34-216-17-40.us-west-2.compute.amazonaws.com\n",
    "Last login: Wed Feb 13 07:39:35 2019\n",
    "\n",
    "       __|  __|_  )\n",
    "       _|  (     /   Amazon Linux AMI\n",
    "      ___|\\___|___|\n",
    "\n",
    "https://aws.amazon.com/amazon-linux-ami/2017.03-release-notes/\n",
    "13 package(s) needed for security, out of 243 available\n",
    "Run \"sudo yum update\" to apply all updates.\n",
    "Amazon Linux version 2018.03 is available.\n",
    "\n",
    "EEEEEEEEEEEEEEEEEEEE MMMMMMMM           MMMMMMMM RRRRRRRRRRRRRRR\n",
    "E::::::::::::::::::E M:::::::M         M:::::::M R::::::::::::::R\n",
    "EE:::::EEEEEEEEE:::E M::::::::M       M::::::::M R:::::RRRRRR:::::R\n",
    "  E::::E       EEEEE M:::::::::M     M:::::::::M RR::::R      R::::R\n",
    "  E::::E             M::::::M:::M   M:::M::::::M   R:::R      R::::R\n",
    "  E:::::EEEEEEEEEE   M:::::M M:::M M:::M M:::::M   R:::RRRRRR:::::R\n",
    "  E::::::::::::::E   M:::::M  M:::M:::M  M:::::M   R:::::::::::RR\n",
    "  E:::::EEEEEEEEEE   M:::::M   M:::::M   M:::::M   R:::RRRRRR::::R\n",
    "  E::::E             M:::::M    M:::M    M:::::M   R:::R      R::::R\n",
    "  E::::E       EEEEE M:::::M     MMM     M:::::M   R:::R      R::::R\n",
    "EE:::::EEEEEEEE::::E M:::::M             M:::::M   R:::R      R::::R\n",
    "E::::::::::::::::::E M:::::M             M:::::M RR::::R      R::::R\n",
    "EEEEEEEEEEEEEEEEEEEE MMMMMMM             MMMMMMM RRRRRRR      RRRRRR\n",
    "\n",
    "[hadoop@ip-172-31-11-173 ~]$\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Your Results\n",
    "\n",
    "Run below cell to get back the notebooks you have run in EMR cluster. A new directory called 'Results' is created in your current directory with all the notebooks you have on EMR cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download all contents of hadoop user to local working directory\n",
    "os.system(\"mkdir \"+wk_dir+\"/results\")\n",
    "os.system(\"scp -o StrictHostKeyChecking=no -r -i \"+wk_dir+\"/\"+emr_pem_file+\".pem hadoop@\"+response['Cluster']['MasterPublicDnsName']+\":/home/hadoop/. \"+wk_dir+\"/results/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terminate Your Cluster\n",
    "Once your work is complete please run the following cells to terminate your cluster and delete your cluster's keypair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Termination Protection\n",
    "emr.set_termination_protection(\n",
    "    JobFlowIds=[\n",
    "        cluster_id,\n",
    "    ],\n",
    "    TerminationProtected=False\n",
    ")\n",
    "# Terminate Cluster\n",
    "response = emr.terminate_job_flows(\n",
    "    JobFlowIds=[\n",
    "       cluster_id ,\n",
    "    ]\n",
    ")\n",
    "print('\\nAWS Metadata: ')\n",
    "print('http Status Code : '+str(response['ResponseMetadata']['HTTPStatusCode']))\n",
    "print('Request ID       : '+response['ResponseMetadata']['RequestId'])\n",
    "print('Retries          : '+str(response['ResponseMetadata']['RetryAttempts']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete SSH Keypair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete SSH Keypair\n",
    "\n",
    "try:\n",
    "    os.remove(emr_pem_file+'.pem')\n",
    "    print('Local Key Deleted')\n",
    "except:\n",
    "    print('Local Key Not Found')\n",
    "    \n",
    "response = ec2.delete_key_pair(KeyName=emr_pem_file)\n",
    "print('\\nAWS Metadata: ')\n",
    "print('http Status Code : '+str(response['ResponseMetadata']['HTTPStatusCode']))\n",
    "print('Request ID       : '+response['ResponseMetadata']['RequestId'])\n",
    "print('Retries          : '+str(response['ResponseMetadata']['RetryAttempts']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your notebook, then `File > Close and Halt`\n",
    "\n",
    "# Cells Beyond this point are for Troubleshooting and Devlopement Only\n",
    " #### Do not publish these cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List SSH Keypairs\n",
    "response = ec2.describe_key_pairs()\n",
    "print(json.dumps(response, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Describe Cluster\n",
    "response = emr.describe_cluster(\n",
    "    ClusterId=cluster_id  \n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually Delete SSH Keypair\n",
    "user_input = input(\"Key Name: \")\n",
    "response = ec2.delete_key_pair(KeyName=user_input)\n",
    "print(json.dumps(response, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get VPC ID\n",
    "security_response = ec2.describe_security_groups(GroupIds=[master_security_group])\n",
    "print(security_response['SecurityGroups'][0]['VpcId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AWS Steps Example  \n",
    "Steps=[\n",
    "  {'Name': 'My word count example',\n",
    "   'HadoopJarStep': {\n",
    "       'Jar': 'command-runner.jar',\n",
    "       'Args': [\n",
    "           'hadoop-streaming',\n",
    "           '-files', 's3://dsabucket1/tweetSplitter.py',\n",
    "           '-mapper', 'python3.4 tweetSplitter.py',\n",
    "           '-input', 's3://dsabucket1/tweets_wc/input/',\n",
    "           '-output', 's3://dsabucket1/tweets_wc/output/results',\n",
    "           '-reducer', 'aggregate']}\n",
    "   }\n",
    "],\n",
    "#Do not run this cell (Standalone) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS Jupyter Install Bootstrap Action \n",
    "\"Name\": \"Install Jupyter notebook\",\n",
    "        \"ScriptBootstrapAction\": { \n",
    "        \"Args\": [\"r\",\n",
    "                 \"julia\",\n",
    "                 \"toree\",\n",
    "                 \"torch\",\n",
    "                 \"ruby\",\n",
    "                 \"Scala\",\n",
    "                 \"PySpark\",\n",
    "                 \"SparkR\",\n",
    "                 \"SQL\",\n",
    "                 \"ds-packages\",\n",
    "                 \"ml-packages\",\n",
    "                 \"python-packages={ggplot,nilearn}\",\n",
    "                 \"port=8880\",\n",
    "                 \"password=jupyter\",\n",
    "                 \"jupyterhub\",\n",
    "                 \"jupyterhub-port=8001\",\n",
    "                 \"cached-install\",\n",
    "                 \"notebook-dir=s3://aws-logs-714861692883-us-east-1/notebooks/copy-samples\",\n",
    "                 \"copy-samples\",\n",
    "                 \"ssh\"\n",
    "                ],\n",
    "       \"Path\": \"s3://aws-bigdata-blog/artifacts/aws-blog-emr-jupyter/install-jupyter-emr5.sh\"\n",
    "}\n",
    "#Do not run this cell (Standalone)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bootstrapping Actions Run on All Nodes\n",
    "  BootstrapActions= [ \n",
    "     { \n",
    "        \"Name\": \"Install Jupyter Notebook\",\n",
    "               \"ScriptBootstrapAction\": { \n",
    "               \"Path\": \"s3://dsa-mizzou/scripts/inst-run_jupyter.sh\"\n",
    "        },\n",
    "        \"Name\": \"Load Reviews Dataset into HDFS\",\n",
    "               \"ScriptBootstrapAction\": { \n",
    "               \"Path\": \"s3://dsa-mizzou/scripts/load_dataset.sh\"\n",
    "        },\n",
    "        \"Name\": \"Load Lession Data\",\n",
    "               \"ScriptBootstrapAction\": { \n",
    "               \"Path\": \"s3://dsa-mizzou/courses/####/####NB_data.sh\"\n",
    "       }\n",
    "     }\n",
    "  ],\n",
    "#Do not run this cell (Standalone)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step Actions Run Only on Master Node  \n",
    "  Steps=[\n",
    "      {'Name': 'Run Installer for Something',\n",
    "       'ActionOnFailure': 'CONTINUE',\n",
    "       'HadoopJarStep': {\n",
    "           'Jar': 's3://region.elasticmapreduce/libs/script-runner/script-runner.jar',\n",
    "           'Args': [\n",
    "               's3://dsa-mizzou/scripts/my-script.sh']}\n",
    "       }\n",
    "  ],\n",
    "#Do not run this cell (Standalone)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
