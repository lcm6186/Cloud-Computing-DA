{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS EMR Launcher\n",
    "Welcome to the DSA Hadoop Launcher. Here you will add your key and a couple paramaters to deploy your own Hadoop stack on AWS.\n",
    "\n",
    "Please set the paramaters in the \"SET THE FOLLOWING PARAMETERS\" box below and run the first cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<botocore.client.EMR object at 0x7fdffd7b5be0>\n",
      "<botocore.client.EC2 object at 0x7fdffd65f198>\n"
     ]
    }
   ],
   "source": [
    "# Launch an EMR cluster with a Jupyter Notebook\n",
    "\n",
    "################################### SET THE FOLLOWING PARAMETERS ###################################################\n",
    "#Set the AWS Region\n",
    "region = 'us-west-2'\n",
    "\n",
    "#Set the AWS Access ID (Given to you buy the DSA staff)\n",
    "access_id = ''\n",
    "\n",
    "#Set the AWS Access Key (Given to you buy the DSA staff)\n",
    "access_key = ''\n",
    "\n",
    "#Set the Size of the AWS EC2 Instances\n",
    "instance_size = 'm3.xlarge'\n",
    "\n",
    "#Set the Number of Master Instances\n",
    "master_instances = 1\n",
    "\n",
    "#Set the Number of Slave Instances\n",
    "slave_instances = 2\n",
    "\n",
    "#Dataset Provided By Your Instructor\n",
    "dataset_location = 'https://s3-us-west-2.amazonaws.com/dataset-store/amazon_reviews/reviews.json'\n",
    "dataset_file_name = 'reviews.json'\n",
    "\n",
    "#Folder Name for Notebooks Transferring to AWS\n",
    "load_notebook_location = 'notebooks'\n",
    "####################################################################################################################\n",
    "\n",
    "\n",
    "#Import AWS Tools\n",
    "import boto3\n",
    "\n",
    "#Establish The EMR Session\n",
    "emr = boto3.client(\n",
    "   'emr',\n",
    "   region_name=region,\n",
    "   aws_access_key_id=access_id,\n",
    "   aws_secret_access_key=access_key\n",
    ")\n",
    "\n",
    "#Establish The EC2 Session\n",
    "ec2 = boto3.client(\n",
    "   'ec2',\n",
    "   region_name=region,\n",
    "   aws_access_key_id=access_id,\n",
    "   aws_secret_access_key=access_key\n",
    ")\n",
    "\n",
    "#Import System Tools\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import getpass\n",
    "from subprocess import call\n",
    "import fabric\n",
    "\n",
    "#Set important Variables\n",
    "system_user_name=getpass.getuser()\n",
    "wk_dir=os.getcwd()\n",
    "\n",
    "print(emr)\n",
    "print(ec2)\n",
    "\n",
    "#Price Calculator Development In Progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create SSH Keypair\n",
    "This will create a temporary keypair for you to access your cluster and save it to your current working directory. \n",
    "\n",
    "This is automatic so please run this cell as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyName         : EMR-13112017143931-skaf48\n",
      "Key Fingerprint : 1c:9f:e7:e0:87:f4:8b:86:7b:9d:b8:00:d6:f1:cd:f8:34:fa:4a:1c\n"
     ]
    }
   ],
   "source": [
    "# Create SSH Keypair File For This EMR Cluster\n",
    "\n",
    "emr_pem_file=time.strftime(\"EMR-%d%m%Y%H%M%S-\"+system_user_name)\n",
    "emr_key=ec2.create_key_pair(KeyName=emr_pem_file)\n",
    "\n",
    "#Don't do this unless you have a good reason\n",
    "#print(emr_key['KeyMaterial'])\n",
    "\n",
    "os.system(\"echo \\\"\"+emr_key['KeyMaterial']+\"\\\" > \"+emr_pem_file+\".pem\")\n",
    "os.chmod(wk_dir+\"/\"+emr_pem_file+\".pem\",0o400)\n",
    "\n",
    "print(\"KeyName         : \"+emr_key['KeyName']+\"\\nKey Fingerprint : \"+emr_key['KeyFingerprint'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch EMR Cluster\n",
    "This step will launch your Hadoop cluster. From this point on you will be charged money for every hour that this cluster is running. Please proceed with caution.\n",
    "\n",
    "All arguments for the following cell have been set in the first cell. Please run the following cell as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Name : EMR Jupyter NB-skaf48\n",
      "Cluster ID   : j-3HB91ZBBX73T5\n",
      "\n",
      "***Please Wait***\n",
      "\n",
      "STARTING................................Cluster DNS Active\n",
      "\n",
      "Proceeding with Firewall Rules...\n",
      "SSH already added\n",
      "YARN already added\n",
      "HDFS NameNode already added\n",
      "Spark History Server already added\n",
      "Hue already added\n",
      "HBase already added\n",
      "Jupyter Notebook already added\n",
      "Slave SSH already added\n",
      "Slave YARN NodeManager already added\n",
      "Slave HDFS DataNode already added\n",
      "\n",
      "\n",
      "Finishing Startup.\n",
      "This will take a few minutes...\n",
      "\n",
      "***Please Wait***\n",
      "\n",
      "Starting.............................................Done\n",
      "\n",
      "Running Bootstrap Actions.\n",
      "This will take a few minutes...\n",
      "\n",
      "***Please Wait***\n",
      "\n",
      "Bootstrapping....Done\n",
      "\n",
      "Cluster Status: WAITING\n",
      "\n",
      "Installing Jupyter...\n",
      "[ec2-54-191-11-124.us-west-2.compute.amazonaws.com] run: sudo -u root pip-3.4 install jupyter\n",
      "[ec2-54-191-11-124.us-west-2.compute.amazonaws.com] run: sudo -u root pip-3.4 install toree\n",
      "[ec2-54-191-11-124.us-west-2.compute.amazonaws.com] run: export SPARK_HOME=/usr/lib/spark;export PYTHONPATH=$PYTHONPATH:$SPARK_HOME/python:$SPARK_HOME/python/lib\n",
      "[ec2-54-191-11-124.us-west-2.compute.amazonaws.com] run: sudo -u root /usr/local/bin/jupyter toree install --replace --spark_home=/usr/lib/spark --spark_opts=\"--master=local[*]\" --interpreters=Scala,PySpark,SparkR,SQL\n",
      "[ec2-54-191-11-124.us-west-2.compute.amazonaws.com] run: mkdir -p /home/hadoop/.jupyter/\n",
      "[ec2-54-191-11-124.us-west-2.compute.amazonaws.com] run: curl -o /home/hadoop/.jupyter/jupyter_notebook_config.py https://s3-us-west-2.amazonaws.com/dsa-mizzou/scripts/jupyter_notebook_config.py\n",
      "[ec2-54-191-11-124.us-west-2.compute.amazonaws.com] run: sudo -u root yum -y install tmux\n",
      "[ec2-54-191-11-124.us-west-2.compute.amazonaws.com] run: tmux new-session -d \"jupyter notebook --no-browser --config /home/hadoop/.jupyter/jupyter_notebook_config.py\"\n",
      "\n",
      "Done\n",
      "\n",
      "Loading Dataset...\n",
      "[ec2-54-191-11-124.us-west-2.compute.amazonaws.com] run: /usr/bin/hadoop fs -mkdir Datasets\n",
      "[ec2-54-191-11-124.us-west-2.compute.amazonaws.com] run: curl https://s3-us-west-2.amazonaws.com/dataset-store/amazon_reviews/reviews.json | hadoop fs -appendToFile - Datasets/reviews.json\n",
      "\n",
      "Done\n",
      "\n",
      "Please Proceed to the Next Step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Launch an EMR cluster\n",
    "\n",
    "response = emr.run_job_flow(\n",
    "   Name='EMR Jupyter NB-'+system_user_name,\n",
    "   LogUri='s3n://logs-'+system_user_name+'/elasticmapreduce/',\n",
    "   ReleaseLabel='emr-4.9.2',\n",
    "   Instances={\n",
    "       \n",
    "       'InstanceGroups': [\n",
    "           {\n",
    "               'Name':'Master - 1',\n",
    "               'InstanceRole':'MASTER',\n",
    "               'InstanceType': instance_size,\n",
    "               'InstanceCount': master_instances\n",
    "           },\n",
    "           {\n",
    "               'Name':'Core - 2',\n",
    "               'InstanceRole':'CORE',\n",
    "               'InstanceType': instance_size,\n",
    "               'InstanceCount': slave_instances\n",
    "           }\n",
    "       ],\n",
    "       'KeepJobFlowAliveWhenNoSteps': True,\n",
    "       'TerminationProtected':True,\n",
    "       'Ec2KeyName': emr_pem_file,\n",
    "       'Placement': {\n",
    "           'AvailabilityZone': 'us-west-2c'\n",
    "       }\n",
    "   },\n",
    "\n",
    "\n",
    "#Insert Steps Here if Applicable \n",
    "\n",
    "#Insert Bootstrapping Actions Here if Applicable\n",
    "\n",
    "   \n",
    "   AutoScalingRole=\"EMR_AutoScaling_DefaultRole\",\n",
    "   Applications=[\n",
    "       {\n",
    "           'Name': 'Hadoop'\n",
    "       },\n",
    "       {\n",
    "           'Name': 'Hive'\n",
    "       },\n",
    "       {\n",
    "           'Name': 'Spark'\n",
    "       },\n",
    "       {\n",
    "           'Name': 'Pig'\n",
    "       }\n",
    "   ],\n",
    "   Configurations=[\n",
    "       {\n",
    "           'Classification': 'spark',\n",
    "           'Configurations': [],\n",
    "           'Properties': {\n",
    "               'maximizeResourceAllocation':'true'\n",
    "           }\n",
    "       },\n",
    "   ],\n",
    "   VisibleToAllUsers=False,\n",
    "   EbsRootVolumeSize=10,\n",
    "   JobFlowRole='EMR_EC2_DefaultRole',\n",
    "   ServiceRole='EMR_DefaultRole',\n",
    "   #ScaleDownBehavior='TERMINATE_AT_INSTANCE_HOUR', #For reliese 5.0.0+\n",
    "    \n",
    ")#End of Cluster Launch Command\n",
    "\n",
    "#Define Cluster ID\n",
    "cluster_id = response['JobFlowId']\n",
    "#Get Cluster Info\n",
    "response = emr.describe_cluster(\n",
    "    ClusterId=cluster_id  \n",
    ")\n",
    "print (\"Cluster Name : \"+response['Cluster']['Name']+\"\\nCluster ID   : \"+response['Cluster']['Id'])\n",
    "\n",
    "#Wait for Bootstrap and Print Cluster Details\n",
    "print (\"\\n***Please Wait***\\n\\n\"+response['Cluster']['Status']['State']+\".\",end=\"\")\n",
    "while True:\n",
    "    response = emr.describe_cluster(\n",
    "        ClusterId=cluster_id  \n",
    "    )\n",
    "    try:\n",
    "        response['Cluster']['MasterPublicDnsName'].find(\"ec2\")\n",
    "        print('...Cluster DNS Active',end=\"\")\n",
    "        break\n",
    "    except:    \n",
    "        time.sleep(5)\n",
    "        print(\".\", end=\"\")\n",
    "        pass\n",
    "\n",
    "print(\"\\n\\nProceeding with Firewall Rules...\")\n",
    "\n",
    "#Get Cluster Security Group Info\n",
    "master_security_group = response['Cluster']['Ec2InstanceAttributes']['EmrManagedMasterSecurityGroup']\n",
    "slave_security_group = response['Cluster']['Ec2InstanceAttributes']['EmrManagedSlaveSecurityGroup']\n",
    "\n",
    "#Create Firewall Exceptions\n",
    "try:\n",
    "    sec_rule=\"SSH\"\n",
    "    data = ec2.authorize_security_group_ingress(\n",
    "        GroupId=master_security_group,\n",
    "        IpPermissions=[\n",
    "            {'IpProtocol': 'tcp',\n",
    "             'FromPort': 22,\n",
    "             'ToPort': 22,\n",
    "             'IpRanges': [{'CidrIp': '128.206.0.0/16'}]},\n",
    "        ])\n",
    "    print(\"Ingress \"+sec_rule+\" added\")\n",
    "except:\n",
    "    print(sec_rule+\" already added\")\n",
    "\n",
    "try:\n",
    "    sec_rule=\"YARN\"\n",
    "    data = ec2.authorize_security_group_ingress(\n",
    "        GroupId=master_security_group,\n",
    "        IpPermissions=[\n",
    "            {'IpProtocol': 'tcp',\n",
    "             'FromPort': 8088,\n",
    "             'ToPort': 8088,\n",
    "             'IpRanges': [{'CidrIp': '128.206.0.0/16'}]},\n",
    "        ])\n",
    "    print(\"Ingress \"+sec_rule+\" added\")\n",
    "except:\n",
    "    print(sec_rule+\" already added\")\n",
    "    \n",
    "try:\n",
    "    sec_rule=\"HDFS NameNode\"\n",
    "    data = ec2.authorize_security_group_ingress(\n",
    "        GroupId=master_security_group,\n",
    "        IpPermissions=[\n",
    "            {'IpProtocol': 'tcp',\n",
    "             'FromPort': 50070,\n",
    "             'ToPort': 50070,\n",
    "             'IpRanges': [{'CidrIp': '128.206.0.0/16'}]},\n",
    "        ])\n",
    "    print(\"Ingress \"+sec_rule+\" added\")\n",
    "except:\n",
    "    print(sec_rule+\" already added\")\n",
    "\n",
    "try:\n",
    "    sec_rule=\"Spark History Server\"\n",
    "    data = ec2.authorize_security_group_ingress(\n",
    "        GroupId=master_security_group,\n",
    "        IpPermissions=[\n",
    "            {'IpProtocol': 'tcp',\n",
    "             'FromPort': 18080,\n",
    "             'ToPort': 18080,\n",
    "             'IpRanges': [{'CidrIp': '128.206.0.0/16'}]},\n",
    "        ])\n",
    "    print(\"Ingress \"+sec_rule+\" added\")\n",
    "except:\n",
    "    print(sec_rule+\" already added\")\n",
    "\n",
    "try:\n",
    "    sec_rule=\"Hue\"\n",
    "    data = ec2.authorize_security_group_ingress(\n",
    "        GroupId=master_security_group,\n",
    "        IpPermissions=[\n",
    "            {'IpProtocol': 'tcp',\n",
    "             'FromPort': 8888,\n",
    "             'ToPort': 8888,\n",
    "             'IpRanges': [{'CidrIp': '128.206.0.0/16'}]},\n",
    "        ])\n",
    "    print(\"Ingress \"+sec_rule+\" added\")\n",
    "except:\n",
    "    print(sec_rule+\" already added\")\n",
    "    \n",
    "try:\n",
    "    sec_rule=\"HBase\"\n",
    "    data = ec2.authorize_security_group_ingress(\n",
    "        GroupId=master_security_group,\n",
    "        IpPermissions=[\n",
    "            {'IpProtocol': 'tcp',\n",
    "             'FromPort': 16010,\n",
    "             'ToPort': 16010,\n",
    "             'IpRanges': [{'CidrIp': '128.206.0.0/16'}]},\n",
    "        ])\n",
    "    print(\"Ingress \"+sec_rule+\" added\")\n",
    "except:\n",
    "    print(sec_rule+\" already added\")\n",
    "\n",
    "try:\n",
    "    sec_rule=\"Jupyter Notebook\"\n",
    "    data = ec2.authorize_security_group_ingress(\n",
    "        GroupId=master_security_group,\n",
    "        IpPermissions=[\n",
    "            {'IpProtocol': 'tcp',\n",
    "             'FromPort': 9090,\n",
    "             'ToPort': 9090,\n",
    "             'IpRanges': [{'CidrIp': '128.206.0.0/16'}]},\n",
    "        ])\n",
    "    print(\"Ingress \"+sec_rule+\" added\")\n",
    "except:\n",
    "    print(sec_rule+\" already added\")\n",
    "\n",
    "try:\n",
    "    sec_rule=\"Slave SSH\"\n",
    "    data = ec2.authorize_security_group_ingress(\n",
    "        GroupId=slave_security_group,\n",
    "        IpPermissions=[\n",
    "            {'IpProtocol': 'tcp',\n",
    "             'FromPort': 22,\n",
    "             'ToPort': 22,\n",
    "             'IpRanges': [{'CidrIp': '128.206.0.0/16'}]},\n",
    "        ])\n",
    "    print(\"Ingress \"+sec_rule+\" added\")\n",
    "except:\n",
    "    print(sec_rule+\" already added\")\n",
    "    \n",
    "try:\n",
    "    sec_rule=\"Slave YARN NodeManager\"\n",
    "    data = ec2.authorize_security_group_ingress(\n",
    "        GroupId=slave_security_group,\n",
    "        IpPermissions=[\n",
    "            {'IpProtocol': 'tcp',\n",
    "             'FromPort': 8042,\n",
    "             'ToPort': 8042,\n",
    "             'IpRanges': [{'CidrIp': '128.206.0.0/16'}]},\n",
    "        ])\n",
    "    print(\"Ingress \"+sec_rule+\" added\")\n",
    "except:\n",
    "    print(sec_rule+\" already added\")\n",
    "    \n",
    "try:\n",
    "    sec_rule=\"Slave HDFS DataNode\"\n",
    "    data = ec2.authorize_security_group_ingress(\n",
    "        GroupId=slave_security_group,\n",
    "        IpPermissions=[\n",
    "            {'IpProtocol': 'tcp',\n",
    "             'FromPort': 50075,\n",
    "             'ToPort': 50075,\n",
    "             'IpRanges': [{'CidrIp': '128.206.0.0/16'}]},\n",
    "        ])\n",
    "    print(\"Ingress \"+sec_rule+\" added\")\n",
    "except:\n",
    "    print(sec_rule+\" already added\")\n",
    "    \n",
    "print (\"\\n\\nFinishing Startup.\\nThis will take a few minutes...\\n\\n***Please Wait***\\n\\nStarting.\",end=\"\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "while str(response['Cluster']['Status']['State']) == 'STARTING':\n",
    "        time.sleep(5)\n",
    "        print(\".\", end=\"\")\n",
    "        response = emr.describe_cluster(\n",
    "            ClusterId=cluster_id  \n",
    "        )\n",
    "print('...Done',end=\"\")  \n",
    "\n",
    "print (\"\\n\\nRunning Bootstrap Actions.\\nThis will take a few minutes...\\n\\n***Please Wait***\\n\\nBootstrapping.\",end=\"\")\n",
    "\n",
    "while str(response['Cluster']['Status']['State']) == 'BOOTSTRAPPING':\n",
    "        time.sleep(5)\n",
    "        print(\".\", end=\"\")\n",
    "        response = emr.describe_cluster(\n",
    "            ClusterId=cluster_id  \n",
    "        )\n",
    "print('...Done',end=\"\")  \n",
    "print('\\n\\nCluster Status: '+response['Cluster']['Status']['State'])\n",
    "\n",
    "#Refresh Cluster Description\n",
    "response = emr.describe_cluster(\n",
    "    ClusterId=cluster_id  \n",
    ")\n",
    "\n",
    "#Bootstrap Cluster with Fabric\n",
    "from fabric import tasks\n",
    "from fabric.api import run\n",
    "from fabric.api import env\n",
    "from fabric.api import hide\n",
    "from fabric.network import disconnect_all\n",
    "\n",
    "env.host_string = response['Cluster']['MasterPublicDnsName']\n",
    "env.user = 'hadoop'\n",
    "env.key_filename = wk_dir+\"/\"+emr_pem_file+\".pem\"\n",
    "env.warn_only\n",
    "\n",
    "def install_jupyter():\n",
    "    with hide('output'):\n",
    "        run('sudo -u root pip-3.4 install jupyter')\n",
    "        run('sudo -u root pip-3.4 install toree')\n",
    "        run('export SPARK_HOME=/usr/lib/spark;export PYTHONPATH=$PYTHONPATH:$SPARK_HOME/python:$SPARK_HOME/python/lib')\n",
    "        run('sudo -u root /usr/local/bin/jupyter toree install --replace --spark_home=/usr/lib/spark --spark_opts=\"--master=local[*]\" --interpreters=Scala,PySpark,SparkR,SQL')\n",
    "        run('mkdir -p /home/hadoop/.jupyter/')\n",
    "        run('curl -o /home/hadoop/.jupyter/jupyter_notebook_config.py https://s3-us-west-2.amazonaws.com/dsa-mizzou/scripts/jupyter_notebook_config.py')\n",
    "        run('sudo -u root yum -y install tmux')\n",
    "        run('tmux new-session -d \"jupyter notebook --no-browser --config /home/hadoop/.jupyter/jupyter_notebook_config.py\"')\n",
    "\n",
    "def load_dataset():\n",
    "    with hide('output'):\n",
    "        run('/usr/bin/hadoop fs -mkdir Datasets')\n",
    "        run('curl '+dataset_location+' | hadoop fs -appendToFile - Datasets/'+dataset_file_name)\n",
    "\n",
    "print('\\nInstalling Jupyter...')        \n",
    "install_jupyter()\n",
    "print('\\nDone\\n')\n",
    "print('Loading Dataset...')\n",
    "load_dataset()\n",
    "print('\\nDone\\n')\n",
    "\n",
    "        \n",
    "#Upload Notebook Directory\n",
    "os.system(\"scp -o StrictHostKeyChecking=no -r -i \"+wk_dir+\"/\"+emr_pem_file+\".pem \"+wk_dir+\"/\"+load_notebook_location+\"/.\"+\" hadoop@\"+response['Cluster']['MasterPublicDnsName']+\":/home/hadoop/\" )\n",
    "\n",
    "print('Please Proceed to the Next Step')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Access your EMR Cluster's Interfaces\n",
    "\n",
    "#### For Web Interfaces Run the Following Cell\n",
    "\n",
    "\n",
    "We are interested in running the Jupyter notebook on cluster. So click on the first link for launching Jupyterhub on EMR cluster. Run the Reddit_commnents.ipynb lab notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter Notebooks\n",
      "http://ec2-54-191-11-124.us-west-2.compute.amazonaws.com:9090/\n",
      "\n",
      "YARN ResourceManager\n",
      "http://ec2-54-191-11-124.us-west-2.compute.amazonaws.com:8088/\n",
      "\n",
      "Hadoop HDFS NameNode\n",
      "http://ec2-54-191-11-124.us-west-2.compute.amazonaws.com:50070/\n",
      "\n",
      "Spark HistoryServer\n",
      "http://ec2-54-191-11-124.us-west-2.compute.amazonaws.com:18080/\n",
      "\n",
      "Hue\n",
      "http://ec2-54-191-11-124.us-west-2.compute.amazonaws.com:8888/\n",
      "\n",
      "Ganglia\n",
      "http://ec2-54-191-11-124.us-west-2.compute.amazonaws.com/ganglia/\n",
      "\n",
      "HBase UI\n",
      "http://ec2-54-191-11-124.us-west-2.compute.amazonaws.com:16010/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Web Addresses to EMR\n",
    "print(\"Jupyter Notebooks\\nhttp://\"+response['Cluster']['MasterPublicDnsName']+\":9090/\\n\")\n",
    "print(\"YARN ResourceManager\\nhttp://\"+response['Cluster']['MasterPublicDnsName']+\":8088/\\n\")\n",
    "print(\"Hadoop HDFS NameNode\\nhttp://\"+response['Cluster']['MasterPublicDnsName']+\":50070/\\n\")\n",
    "print(\"Spark HistoryServer\\nhttp://\"+response['Cluster']['MasterPublicDnsName']+\":18080/\\n\")\n",
    "print(\"Hue\\nhttp://\"+response['Cluster']['MasterPublicDnsName']+\":8888/\\n\")\n",
    "print(\"Ganglia\\nhttp://\"+response['Cluster']['MasterPublicDnsName']+\"/ganglia/\\n\")\n",
    "print(\"HBase UI\\nhttp://\"+response['Cluster']['MasterPublicDnsName']+\":16010/\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not doing anything in the terminal. So yu dont have to worry about doing SSH into the master. Ignore below cell.\n",
    "\n",
    "\n",
    "#### For SSH, Run the Following Cell and See Instructions Below\n",
    " 1. Run the Cell below\n",
    " 1. Highlight the ssh line and press Ctrl+C to copy it to your local clipboard\n",
    " 1. Click the link below to open a termainal\n",
    " 1. Paste the SSH link in (Ctrl + V) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SSH to EMR\n",
    "print(\"ssh -i \"+wk_dir+\"/\"+emr_pem_file+\".pem\"+\" hadoop@\"+response['Cluster']['MasterPublicDnsName'])\n",
    "print(\"https://dev.dsa.missouri.edu/user/\"+system_user_name+\"/terminals/1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Your Results\n",
    "\n",
    "Run below cell to get back the notebooks you have run in EMR cluster. A new directory called 'Results' is created in your current directory with all the notebooks you ran on EMR cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Download all contents of hadoop user to local working directory\n",
    "os.system(\"mkdir \"+wk_dir+\"/results\")\n",
    "os.system(\"scp -o StrictHostKeyChecking=no -r -i \"+wk_dir+\"/\"+emr_pem_file+\".pem hadoop@\"+response['Cluster']['MasterPublicDnsName']+\":/home/hadoop/. \"+wk_dir+\"/results/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terminate Your Cluster\n",
    "Once your work is complete please run the following cells to terminate your cluster and delete your cluster's keypair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove Termination Protection\n",
    "emr.set_termination_protection(\n",
    "    JobFlowIds=[\n",
    "        cluster_id,\n",
    "    ],\n",
    "    TerminationProtected=False\n",
    ")\n",
    "# Terminate Cluster\n",
    "response = emr.terminate_job_flows(\n",
    "    JobFlowIds=[\n",
    "       cluster_id ,\n",
    "    ]\n",
    ")\n",
    "print('\\nAWS Metadata: ')\n",
    "print('http Status Code : '+str(response['ResponseMetadata']['HTTPStatusCode']))\n",
    "print('Request ID       : '+response['ResponseMetadata']['RequestId'])\n",
    "print('Retries          : '+str(response['ResponseMetadata']['RetryAttempts']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete SSH Keypair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Delete SSH Keypair\n",
    "\n",
    "try:\n",
    "    os.remove(emr_pem_file+'.pem')\n",
    "    print('Local Key Deleted')\n",
    "except:\n",
    "    print('Local Key Not Found')\n",
    "    \n",
    "response = ec2.delete_key_pair(KeyName=emr_pem_file)\n",
    "print('\\nAWS Metadata: ')\n",
    "print('http Status Code : '+str(response['ResponseMetadata']['HTTPStatusCode']))\n",
    "print('Request ID       : '+response['ResponseMetadata']['RequestId'])\n",
    "print('Retries          : '+str(response['ResponseMetadata']['RetryAttempts']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Cells Beyond this point are for Troubleshooting and Devlopement Only\n",
    " #### Do not publish these cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List SSH Keypairs\n",
    "response = ec2.describe_key_pairs()\n",
    "print(json.dumps(response, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Describe Cluster\n",
    "response = emr.describe_cluster(\n",
    "    ClusterId=cluster_id  \n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Manually Delete SSH Keypair\n",
    "user_input = input(\"Key Name: \")\n",
    "response = ec2.delete_key_pair(KeyName=user_input)\n",
    "print(json.dumps(response, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get VPC ID\n",
    "security_response = ec2.describe_security_groups(GroupIds=[master_security_group])\n",
    "print(security_response['SecurityGroups'][0]['VpcId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#AWS Steps Example  \n",
    "Steps=[\n",
    "  {'Name': 'My word count example',\n",
    "   'HadoopJarStep': {\n",
    "       'Jar': 'command-runner.jar',\n",
    "       'Args': [\n",
    "           'hadoop-streaming',\n",
    "           '-files', 's3://dsabucket1/tweetSplitter.py',\n",
    "           '-mapper', 'python3.4 tweetSplitter.py',\n",
    "           '-input', 's3://dsabucket1/tweets_wc/input/',\n",
    "           '-output', 's3://dsabucket1/tweets_wc/output/results',\n",
    "           '-reducer', 'aggregate']}\n",
    "   }\n",
    "],\n",
    "#Do not run this cell (Standalone) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# AWS Jupyter Install Bootstrap Action \n",
    "\"Name\": \"Install Jupyter notebook\",\n",
    "        \"ScriptBootstrapAction\": { \n",
    "        \"Args\": [\"r\",\n",
    "                 \"julia\",\n",
    "                 \"toree\",\n",
    "                 \"torch\",\n",
    "                 \"ruby\",\n",
    "                 \"Scala\",\n",
    "                 \"PySpark\",\n",
    "                 \"SparkR\",\n",
    "                 \"SQL\",\n",
    "                 \"ds-packages\",\n",
    "                 \"ml-packages\",\n",
    "                 \"python-packages={ggplot,nilearn}\",\n",
    "                 \"port=8880\",\n",
    "                 \"password=jupyter\",\n",
    "                 \"jupyterhub\",\n",
    "                 \"jupyterhub-port=8001\",\n",
    "                 \"cached-install\",\n",
    "                 \"notebook-dir=s3://aws-logs-714861692883-us-east-1/notebooks/copy-samples\",\n",
    "                 \"copy-samples\",\n",
    "                 \"ssh\"\n",
    "                ],\n",
    "       \"Path\": \"s3://aws-bigdata-blog/artifacts/aws-blog-emr-jupyter/install-jupyter-emr5.sh\"\n",
    "}\n",
    "#Do not run this cell (Standalone)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Bootstrapping Actions Run on All Nodes\n",
    "  BootstrapActions= [ \n",
    "     { \n",
    "        \"Name\": \"Install Jupyter Notebook\",\n",
    "               \"ScriptBootstrapAction\": { \n",
    "               \"Path\": \"s3://dsa-mizzou/scripts/inst-run_jupyter.sh\"\n",
    "        },\n",
    "        \"Name\": \"Load Reviews Dataset into HDFS\",\n",
    "               \"ScriptBootstrapAction\": { \n",
    "               \"Path\": \"s3://dsa-mizzou/scripts/load_dataset.sh\"\n",
    "        },\n",
    "        \"Name\": \"Load Lession Data\",\n",
    "               \"ScriptBootstrapAction\": { \n",
    "               \"Path\": \"s3://dsa-mizzou/courses/####/####NB_data.sh\"\n",
    "       }\n",
    "     }\n",
    "  ],\n",
    "#Do not run this cell (Standalone)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Step Actions Run Only on Master Node  \n",
    "  Steps=[\n",
    "      {'Name': 'Run Installer for Something',\n",
    "       'ActionOnFailure': 'CONTINUE',\n",
    "       'HadoopJarStep': {\n",
    "           'Jar': 's3://region.elasticmapreduce/libs/script-runner/script-runner.jar',\n",
    "           'Args': [\n",
    "               's3://dsa-mizzou/scripts/my-script.sh']}\n",
    "       }\n",
    "  ],\n",
    "#Do not run this cell (Standalone)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
