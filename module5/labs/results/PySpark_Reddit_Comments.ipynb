{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Apache Spark\n",
    "\n",
    "Apache Spark is a framework for distributed computing; this framework aims to\n",
    "make it simpler to write programs that run in parallel across many nodes in a cluster\n",
    "of computers.\n",
    "\n",
    "It is designed from the ground up for high performance in applications of \n",
    "iterative nature, where the same data is accessed multiple times. This performance is\n",
    "achieved primarily through caching datasets in memory, combined with low latency\n",
    "and overhead to launch parallel computation tasks. Together with other features\n",
    "such as fault tolerance, flexible distributed-memory data structures, and a powerful\n",
    "functional API, Spark has proved to be broadly useful for a wide range of large-scale\n",
    "data processing tasks, over and above machine learning and iterative analytics.\n",
    "\n",
    "A Spark cluster is made up of two types of processes: a driver program and multiple\n",
    "executors. In the local mode, all these processes are run within the same JVM. In a\n",
    "cluster, these processes are usually run on separate nodes.\n",
    "\n",
    "\n",
    "#### Components\n",
    "Spark applications run as independent sets of processes on a cluster, coordinated by the SparkContext\n",
    "object in your main program (called the driver program).\n",
    "Specifically, to run on a cluster, the SparkContext can connect to several types of cluster managers (either\n",
    "Spark’s own standalone cluster manager, Mesos or YARN), which allocate resources across applications.\n",
    "Once connected, Spark acquires executors on nodes in the cluster, which are processes that run\n",
    "computations and store data for your application. Next, it sends your application code (defined by JAR or\n",
    "Python files passed to SparkContext) to the executors. Finally, SparkContext sends tasks to the executors to\n",
    "run.\n",
    "\n",
    "\n",
    "<img src=\"../images/spark_components.PNG\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkContext and SparkConf\n",
    "\n",
    "The starting point of writing any Spark program is `SparkContext` (or\n",
    "JavaSparkContext in Java). \n",
    "`SparkContext` is initialized with an instance of a\n",
    "`SparkConf` object, which contains various Spark cluster-configuration settings (for\n",
    "example, the URL of the master node). \n",
    "\n",
    "Once initialized, we will use the various methods found in the `SparkContext` object\n",
    "to create and manipulate distributed datasets and shared variables. \n",
    "The Spark shell (in both Scala and Python, which is unfortunately not supported in Java) takes care\n",
    "of this context initialization for us. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resilient Distributed Datasets\n",
    "\n",
    "The core of Spark is a concept called the *Resilient Distributed Dataset* (RDD).\n",
    "An RDD is a collection of \"records\" (strictly speaking, objects of some type) that is\n",
    "distributed or partitioned across many nodes in a cluster (for the purposes of the\n",
    "Spark local mode, the single multithreaded process can be thought of in the same\n",
    "way). An RDD in Spark is fault-tolerant; this means that if a given node or task fails\n",
    "(for some reason other than erroneous user code, such as hardware failure, loss of\n",
    "communication, and so on), the RDD can be reconstructed automatically on the\n",
    "remaining nodes and the job will still complete.\n",
    "\n",
    "\n",
    "### Creating RDDs\n",
    "\n",
    "RDDs can be created from existing collections, for example, in a Scala Spark shell type as below:\n",
    "    \n",
    "    val collection = List(\"a\", \"b\", \"c\", \"d\", \"e\")\n",
    "    val rddFromCollection = sc.parallelize(collection)\n",
    "    \n",
    "RDDs can also be created from Hadoop-based input sources, including the local\n",
    "filesystem, HDFS, and Amazon S3. The following\n",
    "code is an example of creating an RDD from a text file located on the local filesystem:\n",
    "    \n",
    "    val rddFromTextFile = sc.textFile(\"LICENSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Read this documentation](http://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-overview.html) on AWS website for an overview on AWS EMR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark operations\n",
    "\n",
    "Once we have created an RDD, we have a distributed collection of records that\n",
    "we can manipulate. In Spark's programming model, operations are split into\n",
    "transformations and actions. Generally speaking, a transformation operation applies\n",
    "some function to all the records in the dataset, changing the records in some way.\n",
    "An action typically runs some computation or aggregation operation and returns the\n",
    "result to the driver program where `SparkContext` is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1159, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 985, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1164, in send_command\n",
      "    \"Error while receiving\", e, proto.ERROR_ON_RECEIVE)\n",
      "py4j.protocol.Py4JNetworkError: Error while receiving\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:46379)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-2-269333eb8b1e>\", line 5, in <module>\n",
      "    .appName(\"Reddit-comments\") \\\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 173, in getOrCreate\n",
      "    sc = SparkContext.getOrCreate(sparkConf)\n",
      "  File \"/usr/local/spark/python/pyspark/context.py\", line 367, in getOrCreate\n",
      "    SparkContext(conf=conf or SparkConf())\n",
      "  File \"/usr/local/spark/python/pyspark/context.py\", line 136, in __init__\n",
      "    conf, jsc, profiler_cls)\n",
      "  File \"/usr/local/spark/python/pyspark/context.py\", line 198, in _do_init\n",
      "    self._jsc = jsc or self._initialize_context(self._conf._jconf)\n",
      "  File \"/usr/local/spark/python/pyspark/context.py\", line 306, in _initialize_context\n",
      "    return self._jvm.JavaSparkContext(jconf)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1525, in __call__\n",
      "    answer, self._gateway_client, None, self._fqn)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 336, in get_return_value\n",
      "    format(target_id, \".\", name))\n",
      "py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2040, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:46379)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-2-269333eb8b1e>\", line 5, in <module>\n",
      "    .appName(\"Reddit-comments\") \\\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 173, in getOrCreate\n",
      "    sc = SparkContext.getOrCreate(sparkConf)\n",
      "  File \"/usr/local/spark/python/pyspark/context.py\", line 367, in getOrCreate\n",
      "    SparkContext(conf=conf or SparkConf())\n",
      "  File \"/usr/local/spark/python/pyspark/context.py\", line 136, in __init__\n",
      "    conf, jsc, profiler_cls)\n",
      "  File \"/usr/local/spark/python/pyspark/context.py\", line 198, in _do_init\n",
      "    self._jsc = jsc or self._initialize_context(self._conf._jconf)\n",
      "  File \"/usr/local/spark/python/pyspark/context.py\", line 306, in _initialize_context\n",
      "    return self._jvm.JavaSparkContext(jconf)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1525, in __call__\n",
      "    answer, self._gateway_client, None, self._fqn)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 336, in get_return_value\n",
      "    format(target_id, \".\", name))\n",
      "py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2040, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:46379)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-2-269333eb8b1e>\", line 5, in <module>\n",
      "    .appName(\"Reddit-comments\") \\\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 173, in getOrCreate\n",
      "    sc = SparkContext.getOrCreate(sparkConf)\n",
      "  File \"/usr/local/spark/python/pyspark/context.py\", line 367, in getOrCreate\n",
      "    SparkContext(conf=conf or SparkConf())\n",
      "  File \"/usr/local/spark/python/pyspark/context.py\", line 136, in __init__\n",
      "    conf, jsc, profiler_cls)\n",
      "  File \"/usr/local/spark/python/pyspark/context.py\", line 198, in _do_init\n",
      "    self._jsc = jsc or self._initialize_context(self._conf._jconf)\n",
      "  File \"/usr/local/spark/python/pyspark/context.py\", line 306, in _initialize_context\n",
      "    return self._jvm.JavaSparkContext(jconf)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1525, in __call__\n",
      "    answer, self._gateway_client, None, self._fqn)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 336, in get_return_value\n",
      "    format(target_id, \".\", name))\n",
      "py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2040, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:46379)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-2-269333eb8b1e>\", line 5, in <module>\n",
      "    .appName(\"Reddit-comments\") \\\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 173, in getOrCreate\n",
      "    sc = SparkContext.getOrCreate(sparkConf)\n",
      "  File \"/usr/local/spark/python/pyspark/context.py\", line 367, in getOrCreate\n",
      "    SparkContext(conf=conf or SparkConf())\n",
      "  File \"/usr/local/spark/python/pyspark/context.py\", line 136, in __init__\n",
      "    conf, jsc, profiler_cls)\n",
      "  File \"/usr/local/spark/python/pyspark/context.py\", line 198, in _do_init\n",
      "    self._jsc = jsc or self._initialize_context(self._conf._jconf)\n",
      "  File \"/usr/local/spark/python/pyspark/context.py\", line 306, in _initialize_context\n",
      "    return self._jvm.JavaSparkContext(jconf)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1525, in __call__\n",
      "    answer, self._gateway_client, None, self._fqn)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 336, in get_return_value\n",
      "    format(target_id, \".\", name))\n",
      "py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2040, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:46379)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-2-269333eb8b1e>\", line 5, in <module>\n",
      "    .appName(\"Reddit-comments\") \\\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 173, in getOrCreate\n",
      "    sc = SparkContext.getOrCreate(sparkConf)\n",
      "  File \"/usr/local/spark/python/pyspark/context.py\", line 367, in getOrCreate\n",
      "    SparkContext(conf=conf or SparkConf())\n",
      "  File \"/usr/local/spark/python/pyspark/context.py\", line 136, in __init__\n",
      "    conf, jsc, profiler_cls)\n",
      "  File \"/usr/local/spark/python/pyspark/context.py\", line 198, in _do_init\n",
      "    self._jsc = jsc or self._initialize_context(self._conf._jconf)\n",
      "  File \"/usr/local/spark/python/pyspark/context.py\", line 306, in _initialize_context\n",
      "    return self._jvm.JavaSparkContext(jconf)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1525, in __call__\n",
      "    answer, self._gateway_client, None, self._fqn)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 336, in get_return_value\n",
      "    format(target_id, \".\", name))\n",
      "py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2040, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:46379)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-2-269333eb8b1e>\", line 5, in <module>\n",
      "    .appName(\"Reddit-comments\") \\\n",
      "  File \"/usr/local/spark/python/pyspark/sql/session.py\", line 173, in getOrCreate\n",
      "    sc = SparkContext.getOrCreate(sparkConf)\n",
      "  File \"/usr/local/spark/python/pyspark/context.py\", line 367, in getOrCreate\n",
      "    SparkContext(conf=conf or SparkConf())\n",
      "  File \"/usr/local/spark/python/pyspark/context.py\", line 136, in __init__\n",
      "    conf, jsc, profiler_cls)\n",
      "  File \"/usr/local/spark/python/pyspark/context.py\", line 198, in _do_init\n",
      "    self._jsc = jsc or self._initialize_context(self._conf._jconf)\n",
      "  File \"/usr/local/spark/python/pyspark/context.py\", line 306, in _initialize_context\n",
      "    return self._jvm.JavaSparkContext(jconf)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1525, in __call__\n",
      "    answer, self._gateway_client, None, self._fqn)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 336, in get_return_value\n",
      "    format(target_id, \".\", name))\n",
      "py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2040, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-269333eb8b1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m          \u001b[0;34m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m          \u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Reddit-comments\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m          \u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    171\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                         \u001b[0msparkConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m                     \u001b[0;31m# This SparkContext may be an existing one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[0;32m--> 136\u001b[0;31m                           conf, jsc, profiler_cls)\n\u001b[0m\u001b[1;32m    137\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;31m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m_do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;31m# Create the Java SparkContext through Py4J\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjsc\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0;31m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m_initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0mInitialize\u001b[0m \u001b[0mSparkContext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mto\u001b[0m \u001b[0mallow\u001b[0m \u001b[0msubclass\u001b[0m \u001b[0mspecific\u001b[0m \u001b[0minitialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \"\"\"\n\u001b[0;32m--> 306\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJavaSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1523\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1525\u001b[0;31m             answer, self._gateway_client, None, self._fqn)\n\u001b[0m\u001b[1;32m   1526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1527\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    334\u001b[0m             raise Py4JError(\n\u001b[1;32m    335\u001b[0m                 \u001b[0;34m\"An error occurred while calling {0}{1}{2}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                 format(target_id, \".\", name))\n\u001b[0m\u001b[1;32m    337\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "         .master(\"local\") \\\n",
    "         .appName(\"Reddit-comments\") \\\n",
    "         .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It provide access to many of the underlying structures used by pySpark.\n",
    "\n",
    "The entry point into all SQL functionality in Spark is the SQLContext class. To create a basic instance, all we need is a SparkContext reference. Since we are running Spark in shell mode (using pySpark) we can use the global context object sc for this purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll then create an RDD using sc.parallelize with 20 partitions which will be distributed amongst the Spark Worker nodes and also verify the number of partitions in the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(range(1000), 20) \n",
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at the first five records using the take action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s now perform a few transformations on the RDD which will bin the numbers into the lowest 100s and count the total frequency for each bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.map(lambda r: (round(r/100)*100, 1))\\\n",
    "  .reduceByKey(lambda x,y: x+y)\\\n",
    "  .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with DataFrames and Spark SQL\n",
    "\n",
    "Let’s now try to read some data using the Spark SQL Context. This makes parsing JSON files significantly easier than before. After the reading the parsed data in, the resulting output is a Spark DataFrame. We can then register this as a table and run SQL queries off of it for simple analytics.\n",
    "The data we will be working with is a subset of the reddit comments. The current example is processing reddit comments collected in May 2011 which is roughly 4.8GB.\n",
    "\n",
    "\n",
    "In this example we will calculate the number of distinct authors and the average score of all the comments in each subreddit for the month of May, 2011. The results will then be ranked by the number of distinct authors per subreddit and the average score of all the comments per subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now read in all the comments from May, 2011 using the Spark SQL Context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [StructField(\"archived\", BooleanType(), True), \n",
    "          StructField(\"author\", StringType(), True), \n",
    "          StructField(\"author_flair_css_class\", StringType(), True), \n",
    "          StructField(\"author_flair_text\", StringType(), True), \n",
    "          StructField(\"body\", StringType(), True), \n",
    "          StructField(\"controversiality\", LongType(), True), \n",
    "          StructField(\"created_utc\", StringType(), True), \n",
    "          StructField(\"distinguished\", StringType(), True), \n",
    "          StructField(\"downs\", LongType(), True), \n",
    "          StructField(\"edited\", StringType(), True), \n",
    "          StructField(\"gilded\", LongType(), True), \n",
    "          StructField(\"id\", StringType(), True), \n",
    "          StructField(\"link_id\", StringType(), True), \n",
    "          StructField(\"name\", StringType(), True), \n",
    "          StructField(\"parent_id\", StringType(), True), \n",
    "          StructField(\"retrieved_on\", LongType(), True), \n",
    "          StructField(\"score\", LongType(), True), \n",
    "          StructField(\"score_hidden\", BooleanType(), True), \n",
    "          StructField(\"subreddit\", StringType(), True), \n",
    "          StructField(\"subreddit_id\", StringType(), True), \n",
    "          StructField(\"ups\", LongType(), True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SparkDF = spark.read.json(\"/dsa/data/all_datasets/RC_2011-05\", StructType(fields)).persist(pyspark.StorageLevel.MEMORY_AND_DISK_SER).registerTempTable(\"comments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are first defining the schema of the JSON file. Not defining this is also an option; however, Spark will then need to pass through the data twice to:\n",
    "    \n",
    "    infer the schema\n",
    "\n",
    "    parse the data into a Spark DataFrame\n",
    "    \n",
    "----\n",
    "\n",
    "This can be very time consuming when datasets grow much larger. Since we know what the schema will be for this static dataset, it is in our best interest to define it beforehand. Allowing Spark to infer the schema is particularly useful, however, for scenarios when schemas change over time and fields are added or removed.\n",
    "\n",
    "\n",
    "Next the data is read from the DSA server reddit-comments bucket as a Spark DataFrame using <span style=\"color:#a5541a\"><b>sqlContext.read.json(\"...\")</b></span>. Manipulations on the Spark DataFrame in most cases are significantly more efficient that working with the core RDDs.\n",
    "\n",
    "\n",
    "After reading in the data, we would also like to persist it into memory and disk for multiple uses later on with <span style=\"color:#a5541a\"><b>.persist(StorageLevel.MEMORY_AND_DISK_SER)</b></span>. Choosing the memory and disk option permits Spark to gracefully spill the data to disk if it is too large for memory across all the Spark Worker nodes. \n",
    "\n",
    "Here we will be executing two queries on the dataset. The second query will be able to read directly from the persisted data instead of having to read in the entire dataset again.\n",
    "\n",
    "\n",
    "Lastly the Spark DataFrame is registered as a table with <span style=\"color:#a5541a\"><b>.registerTempTable(\"comments\")</b></span>, so we can run SQL queries off of it. The table can then be referenced by the name \"comments\".\n",
    "\n",
    "\n",
    "Let’s now run some SQL queries on the dataset to find the total number of distinct authors and the average comment score per subreddit for this month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_authors_by_subreddit = sqlContext.sql(\"\"\" \n",
    "    SELECT subreddit, COUNT(DISTINCT author) as authors \n",
    "    FROM comments \n",
    "    GROUP BY subreddit \n",
    "    ORDER BY authors DESC \n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_score_by_subreddit = sqlContext.sql(\"\"\" \n",
    "    SELECT subreddit, AVG(score) as avg_score \n",
    "    FROM comments \n",
    "    GROUP BY subreddit \n",
    "    ORDER BY avg_score DESC \n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s take a look at the top 5 subreddits with the most authors commenting and highest average comment score. Note that every command until now has been a transformation and no data has actually flowed through this point. We have essentially been building a Directed Acyclic Graph (DAG) for the operations to perform on the data. Data only begins flowing through when an action is called such as .collect(), .take(), .first(), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_authors_by_subreddit.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is the first action taken, all the 4.8GB will be read in and parsed from DSA server. \n",
    "<span style=\"background:yellow\">This should take few minutes.</span>\n",
    "\n",
    "\n",
    "You will notice that the next action finishes in about 30 seconds. \n",
    "This is because Spark knows that the original data is persisted into memory and disk and it does not need to go to DSA server to get the data. \n",
    "Had we not persisted the data at the very beginning, this action would take also few minutes minutes (30X slower)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_score_by_subreddit.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting our results is quite straightforward. \n",
    "We can use the .toPandas() function to bring the results back to the Spark Driver as a Pandas DataFrame instead of a Spark DataFrame. \n",
    "We can then work with the DataFrame as we normally do in Pandas.  \n",
    "**Note**: By bringing the data back into a Pandas DataFrame, we are collecting the data into the local memory of the notebook.\n",
    "Not the cluster computing system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "distinct_authors = distinct_authors_by_subreddit.toPandas()  \n",
    "distinct_authors[:20].plot(x='subreddit', y='authors', kind='barh', \n",
    "                           alpha=0.7 , legend = False, title = 'Distinct Authors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_score = average_score_by_subreddit.toPandas()  \n",
    "subreddit_score[:20].plot(x='subreddit', y='avg_score', kind='barh', \n",
    "                          alpha=0.7 , legend = False, title = 'Subreddit Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By stopping the `SparkContext`, we are releasing the resources back the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You are done! Save your notebook, then `File > Close and Halt`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
