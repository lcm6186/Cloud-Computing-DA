{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Apache Spark\n",
    "\n",
    "Apache Spark is a framework for distributed computing; this framework aims to\n",
    "make it simpler to write programs that run in parallel across many nodes in a cluster\n",
    "of computers.\n",
    "\n",
    "It is designed from the ground up for high performance in applications of \n",
    "iterative nature, where the same data is accessed multiple times. This performance is\n",
    "achieved primarily through caching datasets in memory, combined with low latency\n",
    "and overhead to launch parallel computation tasks. Together with other features\n",
    "such as fault tolerance, flexible distributed-memory data structures, and a powerful\n",
    "functional API, Spark has proved to be broadly useful for a wide range of large-scale\n",
    "data processing tasks, over and above machine learning and iterative analytics.\n",
    "\n",
    "A Spark cluster is made up of two types of processes: a driver program and multiple\n",
    "executors. In the local mode, all these processes are run within the same JVM. In a\n",
    "cluster, these processes are usually run on separate nodes.\n",
    "\n",
    "\n",
    "#### Components\n",
    "Spark applications run as independent sets of processes on a cluster, coordinated by the SparkContext\n",
    "object in your main program (called the driver program).\n",
    "Specifically, to run on a cluster, the SparkContext can connect to several types of cluster managers (either\n",
    "Spark’s own standalone cluster manager, Mesos or YARN), which allocate resources across applications.\n",
    "Once connected, Spark acquires executors on nodes in the cluster, which are processes that run\n",
    "computations and store data for your application. Next, it sends your application code (defined by JAR or\n",
    "Python files passed to SparkContext) to the executors. Finally, SparkContext sends tasks to the executors to\n",
    "run.\n",
    "\n",
    "\n",
    "<img src=\"../images/spark_components.PNG\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkContext and SparkConf\n",
    "\n",
    "The starting point of writing any Spark program is `SparkContext` (or\n",
    "JavaSparkContext in Java). \n",
    "`SparkContext` is initialized with an instance of a\n",
    "`SparkConf` object, which contains various Spark cluster-configuration settings (for\n",
    "example, the URL of the master node). \n",
    "\n",
    "Once initialized, we will use the various methods found in the `SparkContext` object\n",
    "to create and manipulate distributed datasets and shared variables. \n",
    "The Spark shell (in both Scala and Python, which is unfortunately not supported in Java) takes care\n",
    "of this context initialization for us. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resilient Distributed Datasets\n",
    "\n",
    "The core of Spark is a concept called the *Resilient Distributed Dataset* (RDD).\n",
    "An RDD is a collection of \"records\" (strictly speaking, objects of some type) that is\n",
    "distributed or partitioned across many nodes in a cluster (for the purposes of the\n",
    "Spark local mode, the single multithreaded process can be thought of in the same\n",
    "way). An RDD in Spark is fault-tolerant; this means that if a given node or task fails\n",
    "(for some reason other than erroneous user code, such as hardware failure, loss of\n",
    "communication, and so on), the RDD can be reconstructed automatically on the\n",
    "remaining nodes and the job will still complete.\n",
    "\n",
    "\n",
    "### Creating RDDs\n",
    "\n",
    "RDDs can be created from existing collections, for example, in a Scala Spark shell type as below:\n",
    "    \n",
    "    val collection = List(\"a\", \"b\", \"c\", \"d\", \"e\")\n",
    "    val rddFromCollection = sc.parallelize(collection)\n",
    "    \n",
    "RDDs can also be created from Hadoop-based input sources, including the local\n",
    "filesystem, HDFS, and Amazon S3. The following\n",
    "code is an example of creating an RDD from a text file located on the local filesystem:\n",
    "    \n",
    "    val rddFromTextFile = sc.textFile(\"LICENSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Read this documentation](http://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-overview.html) on AWS website for an overview on AWS EMR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark operations\n",
    "\n",
    "Once we have created an RDD, we have a distributed collection of records that\n",
    "we can manipulate. In Spark's programming model, operations are split into\n",
    "transformations and actions. Generally speaking, a transformation operation applies\n",
    "some function to all the records in the dataset, changing the records in some way.\n",
    "An action typically runs some computation or aggregation operation and returns the\n",
    "result to the driver program where `SparkContext` is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/0e/4528c9703863fc4df49fd4425c6f15ee5f370cff9e43cea3f8076b034e3f/pyspark-3.2.0.tar.gz (281.3MB)\n",
      "\u001b[K     |████████████████████████████████| 281.3MB 223kB/s  eta 0:00:01    |████████▊                       | 76.8MB 38.6MB/s eta 0:00:06     |███████████████▉                | 139.6MB 53.6MB/s eta 0:00:03\n",
      "\u001b[?25hCollecting py4j==0.10.9.2 (from pyspark)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/e2/543019a6e620b759a59f134158b4595766f9bf520a1081a2ba1a1809ba32/py4j-0.10.9.2-py2.py3-none-any.whl (198kB)\n",
      "\u001b[K     |████████████████████████████████| 204kB 16.8MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.2.0-py2.py3-none-any.whl size=281805912 sha256=c1ca72e00221e1156f348989134925c293b540b8da054ae05a71d986b1a0c22c\n",
      "  Stored in directory: /home/lcmhng/.cache/pip/wheels/e2/8e/91/cf48adee149561846ba332c9c6bb8207385a76c5e8e67c197b\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.2 pyspark-3.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-269333eb8b1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m          \u001b[0;34m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m          \u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Reddit-comments\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m          \u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m                             \u001b[0msparkConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                         \u001b[0;31m# This SparkContext may be an existing one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m                     \u001b[0;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                     \u001b[0;31m# by all sessions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Java gateway process exited before sending its port number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Java gateway process exited before sending its port number"
=======
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-269333eb8b1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m          \u001b[0;34m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m          \u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Reddit-comments\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
>>>>>>> 859282b4f51ec07462ad82c4dbc9eb73cf3583d8
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "         .master(\"local\") \\\n",
    "         .appName(\"Reddit-comments\") \\\n",
    "         .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It provide access to many of the underlying structures used by pySpark.\n",
    "\n",
    "The entry point into all SQL functionality in Spark is the SQLContext class. To create a basic instance, all we need is a SparkContext reference. Since we are running Spark in shell mode (using pySpark) we can use the global context object sc for this purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll then create an RDD using sc.parallelize with 20 partitions which will be distributed amongst the Spark Worker nodes and also verify the number of partitions in the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(range(1000), 20) \n",
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at the first five records using the take action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s now perform a few transformations on the RDD which will bin the numbers into the lowest 100s and count the total frequency for each bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.map(lambda r: (round(r/100)*100, 1))\\\n",
    "  .reduceByKey(lambda x,y: x+y)\\\n",
    "  .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with DataFrames and Spark SQL\n",
    "\n",
    "Let’s now try to read some data using the Spark SQL Context. This makes parsing JSON files significantly easier than before. After the reading the parsed data in, the resulting output is a Spark DataFrame. We can then register this as a table and run SQL queries off of it for simple analytics.\n",
    "The data we will be working with is a subset of the reddit comments. The current example is processing reddit comments collected in May 2011 which is roughly 4.8GB.\n",
    "\n",
    "\n",
    "In this example we will calculate the number of distinct authors and the average score of all the comments in each subreddit for the month of May, 2011. The results will then be ranked by the number of distinct authors per subreddit and the average score of all the comments per subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now read in all the comments from May, 2011 using the Spark SQL Context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [StructField(\"archived\", BooleanType(), True), \n",
    "          StructField(\"author\", StringType(), True), \n",
    "          StructField(\"author_flair_css_class\", StringType(), True), \n",
    "          StructField(\"author_flair_text\", StringType(), True), \n",
    "          StructField(\"body\", StringType(), True), \n",
    "          StructField(\"controversiality\", LongType(), True), \n",
    "          StructField(\"created_utc\", StringType(), True), \n",
    "          StructField(\"distinguished\", StringType(), True), \n",
    "          StructField(\"downs\", LongType(), True), \n",
    "          StructField(\"edited\", StringType(), True), \n",
    "          StructField(\"gilded\", LongType(), True), \n",
    "          StructField(\"id\", StringType(), True), \n",
    "          StructField(\"link_id\", StringType(), True), \n",
    "          StructField(\"name\", StringType(), True), \n",
    "          StructField(\"parent_id\", StringType(), True), \n",
    "          StructField(\"retrieved_on\", LongType(), True), \n",
    "          StructField(\"score\", LongType(), True), \n",
    "          StructField(\"score_hidden\", BooleanType(), True), \n",
    "          StructField(\"subreddit\", StringType(), True), \n",
    "          StructField(\"subreddit_id\", StringType(), True), \n",
    "          StructField(\"ups\", LongType(), True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SparkDF = spark.read.json(\"/dsa/data/all_datasets/RC_2011-05\", StructType(fields)).persist(pyspark.StorageLevel.MEMORY_AND_DISK_SER).registerTempTable(\"comments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are first defining the schema of the JSON file. Not defining this is also an option; however, Spark will then need to pass through the data twice to:\n",
    "    \n",
    "    infer the schema\n",
    "\n",
    "    parse the data into a Spark DataFrame\n",
    "    \n",
    "----\n",
    "\n",
    "This can be very time consuming when datasets grow much larger. Since we know what the schema will be for this static dataset, it is in our best interest to define it beforehand. Allowing Spark to infer the schema is particularly useful, however, for scenarios when schemas change over time and fields are added or removed.\n",
    "\n",
    "\n",
    "Next the data is read from the DSA server reddit-comments bucket as a Spark DataFrame using <span style=\"color:#a5541a\"><b>sqlContext.read.json(\"...\")</b></span>. Manipulations on the Spark DataFrame in most cases are significantly more efficient that working with the core RDDs.\n",
    "\n",
    "\n",
    "After reading in the data, we would also like to persist it into memory and disk for multiple uses later on with <span style=\"color:#a5541a\"><b>.persist(StorageLevel.MEMORY_AND_DISK_SER)</b></span>. Choosing the memory and disk option permits Spark to gracefully spill the data to disk if it is too large for memory across all the Spark Worker nodes. \n",
    "\n",
    "Here we will be executing two queries on the dataset. The second query will be able to read directly from the persisted data instead of having to read in the entire dataset again.\n",
    "\n",
    "\n",
    "Lastly the Spark DataFrame is registered as a table with <span style=\"color:#a5541a\"><b>.registerTempTable(\"comments\")</b></span>, so we can run SQL queries off of it. The table can then be referenced by the name \"comments\".\n",
    "\n",
    "\n",
    "Let’s now run some SQL queries on the dataset to find the total number of distinct authors and the average comment score per subreddit for this month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_authors_by_subreddit = sqlContext.sql(\"\"\" \n",
    "    SELECT subreddit, COUNT(DISTINCT author) as authors \n",
    "    FROM comments \n",
    "    GROUP BY subreddit \n",
    "    ORDER BY authors DESC \n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_score_by_subreddit = sqlContext.sql(\"\"\" \n",
    "    SELECT subreddit, AVG(score) as avg_score \n",
    "    FROM comments \n",
    "    GROUP BY subreddit \n",
    "    ORDER BY avg_score DESC \n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s take a look at the top 5 subreddits with the most authors commenting and highest average comment score. Note that every command until now has been a transformation and no data has actually flowed through this point. We have essentially been building a Directed Acyclic Graph (DAG) for the operations to perform on the data. Data only begins flowing through when an action is called such as .collect(), .take(), .first(), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_authors_by_subreddit.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is the first action taken, all the 4.8GB will be read in and parsed from DSA server. \n",
    "<span style=\"background:yellow\">This should take few minutes.</span>\n",
    "\n",
    "\n",
    "You will notice that the next action finishes in about 30 seconds. \n",
    "This is because Spark knows that the original data is persisted into memory and disk and it does not need to go to DSA server to get the data. \n",
    "Had we not persisted the data at the very beginning, this action would take also few minutes minutes (30X slower)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_score_by_subreddit.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting our results is quite straightforward. \n",
    "We can use the .toPandas() function to bring the results back to the Spark Driver as a Pandas DataFrame instead of a Spark DataFrame. \n",
    "We can then work with the DataFrame as we normally do in Pandas.  \n",
    "**Note**: By bringing the data back into a Pandas DataFrame, we are collecting the data into the local memory of the notebook.\n",
    "Not the cluster computing system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "distinct_authors = distinct_authors_by_subreddit.toPandas()  \n",
    "distinct_authors[:20].plot(x='subreddit', y='authors', kind='barh', \n",
    "                           alpha=0.7 , legend = False, title = 'Distinct Authors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_score = average_score_by_subreddit.toPandas()  \n",
    "subreddit_score[:20].plot(x='subreddit', y='avg_score', kind='barh', \n",
    "                          alpha=0.7 , legend = False, title = 'Subreddit Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By stopping the `SparkContext`, we are releasing the resources back the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You are done! Save your notebook, then `File > Close and Halt`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
