{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing in Spark\n",
    "  \n",
    "This notebook continues exploring spark to perform data processing in a similar manner to your previous experience with Pandas in Python. We will use the airline data, which has been stored in HDFS on the EMR cluster. It is accesible from the Spark cluster. You will be asked to solve some simple problems at the end. There are some challenge activities at the end that you can try to answer. They will have an extra credit. \n",
    "\n",
    "Coming back to Spark, it is a cluster computing system that leverages Hadoop technologies like HDFS for high performance storage and Yarn for cluster management. While some may see Spark as a replacement for Hadoop, an alternative argument can be made that Spark is simply another compute engine for Hadoop, in addition to Map-Reduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "\n",
    "Although we are starting a new sparkContext here, always make sure any SparkContext previously used by the Jupyter Server should be properly released before starting a new one. Lets initialize a new SparkContext to interact with the Spark cluster.\n",
    "\n",
    "----- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "         .master(\"local\") \\\n",
    "         .appName(\"flights-practice\") \\\n",
    "         .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Spark\n",
    "\n",
    "Spark is a framework for processing large-data tasks, in general this means Petabytes (or more of data). Spark can run on the HDFS file system, which can be set up to chunk files into blocks and to replicate these blocks across a cluster's storage to promote increased performance. Spark abstracts these details, however, allowing us to develop an application on a small system and scale up to large data on a cluster. \n",
    "\n",
    "In Spark, communications move between a driver process and the execution processes. This communication is handled for us by using a `SparkContext`, which requests resources from the Spark master process, such as number of cores, which are reserved to complete our Spark tasks. Once a Spark Context is active, we can use the Spark Console to monitor jobs and the overall Spark infrastructure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = range(50)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myRDD = sc.parallelize(data, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous code cell, we created a parallelized collection by using the parallelize method, which partitions the data across cores in a cluster. The general rule indicated in the Spark documentation is that you want 2-4 partitions per core.\n",
    "\n",
    "\n",
    "Next, we use several functions on the RDD to obtain the RDD unique ID, which indicates when new RDDs are created, as well as naming RDDs to view them easily in the Spark cluster management software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Initial RDD id: {0}\".format(myRDD.id()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myRDD.setName(\"DSA RDD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Now, given this simple RDD, we can apply a transformation, in this case\n",
    "we simply add one to each element in the RDD. This tranformation doesn't\n",
    "actually happen until we call an action method, which first occurs in\n",
    "the third code cell below when we call the `collect` method. The new RDD\n",
    "has been created, however as indicated by its new id.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myaddRDD = myRDD.map(lambda a: a + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(myaddRDD.toDebugString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(myaddRDD.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "We can now apply a second transformation, in this case we apply a\n",
    "filter, which selects values from the RDD based on a condition (in this\n",
    "example we select valus that are evenly divisible by 5). The\n",
    "transformation doesn't occur, however, until we once again call the\n",
    "`collect` method, which _collects_ the results of the different\n",
    "transformations.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myfilterRDD = myaddRDD.filter(lambda x: (x % 5) == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myfilterRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(myfilterRDD.toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Tranformations, however, can be chained together in a process called\n",
    "pipelining. Doing so can produce long code strings, which can be\n",
    "difficult to follow (or debug). Thus, it is considered good style\n",
    "to break pipelined operations such that each transformation occurs on a\n",
    "separate line. The following code combines the previous Spark tasks\n",
    "together into a single line, but shown using recommended style.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(sc\n",
    " .parallelize(data)\n",
    " .map(lambda x: x + 1)\n",
    " .filter(lambda x: (x % 5) == 0)\n",
    " .collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Data Processing\n",
    "\n",
    "Previously in this Notebook, we have used Spark to create simple RDDs\n",
    "that demonstrated Spark transformations and actions on small data. Now\n",
    "we will change approaches and analyze the airline data, first starting\n",
    "with the single 2001 flight data file. We can create a new RDD by\n",
    "reading in the data as a textfile, after which we execute the RDD\n",
    "creation by counting the number of lines in the RDD. We subsequently\n",
    "apply several other RDD methods to display the first few rows of data by\n",
    "using the `take` method. Finally, we use the built-in `help` to see the\n",
    "list of supported RDD methods.\n",
    "\n",
    "-----\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = '/dsa/data/all_datasets/flights.csv'\n",
    "\n",
    "text_file = sc.textFile(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_file.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_file.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Display help info on spark rdd\n",
    "help(text_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "With this text RDD, we can begin to process the data. Since our data is,\n",
    "at this point, simply a list of strings, we first need to transform the\n",
    "data into columns, remove the header row, and extract out the columns of\n",
    "interest. These steps are pipelined to create a single RDD, that isn't\n",
    "processed until we execute an action method, in this case, the `first`\n",
    "method that displays the first row in the new RDD.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_data = text_file.map(lambda l: l.split(\",\")) \\\n",
    "            .map(lambda p: (p[0], p[1], p[2], p[7], p[8], p[10], p[11], p[17], p[22])) \\\n",
    "            .filter(lambda line: 'YEAR' not in line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_data.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_data.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of rows in col_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Spark, unlike Pandas, will not handle NA values. Thus we need an\n",
    "additional tranform to remove lines from our RDD that contain missing\n",
    "data. We can accomplish this by using an appropriate filter.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = col_data.filter(lambda line: '' not in line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of rows in col_data which doesn't have any null values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of rows in col_data which have null values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "na_rows = col_data.filter(lambda line: '' in line)\n",
    "na_rows.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the sum of both counts rows with null values and rows free of null values is equal to total number of rows in col_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "5714008+105071"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "To analyze these data, however, we need to convert the columns to the\n",
    "appropriate data types. In this case, we can simply apply one final\n",
    "transformation.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fields = cols.map(lambda p: (int(p[0]), int(p[1]), int(p[2]), p[3],\n",
    "                          p[4], int(p[5]), int(p[6]), int(p[7]), int(p[8])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fields.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Spark DataFrame\n",
    "\n",
    "Spark supports a simplified [Data Frame][spdf] as part of the [Spark\n",
    "SQL][spsql] library. We can create a Data Frame from an existing RDD by\n",
    "also specifying the column labels and data types. The data types must\n",
    "be one of the pre-defined [Spark SQL types][spdt]. After creating the\n",
    "new DataFrame (which is backed by an RDD), we can perform many of the\n",
    "same tasks with Spark that we performed with Pandas (but not all, and\n",
    "not in as simple of an approach). The following code cells show how we\n",
    "can take our 2001 flight data RDD and create a new Data Frame, which we\n",
    "subsequently use in several subsequent code cells.\n",
    "\n",
    "-----\n",
    "[spdf]: https://spark.apache.org/docs/latest/sql-programming-guide.html#dataframes\n",
    "[spsql]: https://spark.apache.org/sql/\n",
    "[spdt]: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# sc is an existing SparkContext.\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "schemaString = \"Year Month DayOfMonth Origin Destination DepTime DepDelay Distance ArrDelay\"\n",
    "\n",
    "fieldTypes = [IntegerType(), IntegerType(), IntegerType(), \\\n",
    "              StringType(), StringType(), IntegerType(), \\\n",
    "              IntegerType(), IntegerType(), IntegerType()]\n",
    "\n",
    "f_data = [StructField(field_name, field_type, True) \\\n",
    "          for field_name, field_type in zip(schemaString.split(), fieldTypes)]\n",
    "\n",
    "schema = StructType(f_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.createDataFrame(fields, schema)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "In the following three code cells, we `show` the first few lines of the\n",
    "DataFrame, then use the `head` method, which displays more semantic\n",
    "information for each row, and finally use the `describe` method, which\n",
    "doesn't execute until the `show` action is invoked. While the output is\n",
    "less visually attractive than the Pandas result, we still obtain the\n",
    "necessary information.\n",
    "\n",
    "After these code cells, we access the DataFrame schema, first by using\n",
    "the `printSchema` method to nicely output the schema, and next access a\n",
    "column directly, which we can now do since we have named our DataFrame\n",
    "columns.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "We can extract data from the DataFrame by using similar techniques to\n",
    "what we used with Pandas. One difference is that we need to `filter` the\n",
    "DataFrame, as opposed to directly accessing rows. However, we can filter\n",
    "rows to extract flights that left O'Hare, and secondly those flights\n",
    "that left O'Hare more than two hours late. In the second case, we also\n",
    "tranform the output to `select` the _Destination_ column and a new\n",
    "column that is the _Distance_ in kilometers.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.filter(df['Origin'] == 'ORD').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.filter(df['Origin'] == 'ORD').filter(df['DepDelay'] > 120).select(df['Destination'], df['Distance'] * 1.6).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Spark SQL\n",
    "\n",
    "Given a Spark DataFrame, we can apply SQL statements directly against\n",
    "the DataFrame by registering the DataFrame as a Spark temporary SQL\n",
    "table. The following code cells demonstrates this, as we register our\n",
    "DataFrame as a `flights` table, and execute a SQL statement to select\n",
    "the same data we obtained from our previous DataFrame filter.Since the\n",
    "data are unordered, we have different results displayed via the `show`\n",
    "method.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.createDataFrame(fields, schema)\n",
    "\n",
    "df.registerTempTable(\"flights\")\n",
    "\n",
    "# SQL can be run over DataFrames that have been registered as a table.\n",
    "sql_q = \"SELECT Destination, Distance FROM flights WHERE Origin = 'ORD' AND DepDelay > 120\"\n",
    "\n",
    "results = sqlContext.sql(sql_q)\n",
    "\n",
    "# The results of SQL queries are RDDs and support all the normal RDD operations.\n",
    "results.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Spark Statistics\n",
    "\n",
    "The simplest type of data analysis is to compute basic statistical\n",
    "measures of sequences of data. The Spark MLlib package includes a \n",
    "[basic statistical][sbs] component that can be easily used to obtain\n",
    "statistical measurements of multiple columns in a Spark RDD. We\n",
    "demonstrate this in the following code cells, where we create an RDD\n",
    "from numeric columns in our `fields` RDD. We use the `colStats` function\n",
    "from the `Statistics` object to compute a range of statistical measures\n",
    "in one pass for all columns in the `sdt` RDD. In the second code cell,\n",
    "we simply provide a nicely formatted display of these quantities for\n",
    "each column.\n",
    "\n",
    "-----\n",
    "\n",
    "[sbs]: https://spark.apache.org/docs/latest/mllib-statistics.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "# Extract numeric columns and compute statistics\n",
    "sdt = fields.map(lambda p: (p[2], p[5], p[6], p[7], p[8]))\n",
    "summary = Statistics.colStats(sdt)\n",
    "\n",
    "# Extract individual statistics for RDD\n",
    "mus = summary.mean()\n",
    "mns = summary.min()\n",
    "mxs = summary.max()\n",
    "vrs = summary.variance()\n",
    "nnzs = summary.numNonzeros()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Labels for display\n",
    "cols = ['Day', 'Dep. Time', 'Dep. Delay', 'Distance', 'Arr. Delay']\n",
    "\n",
    "# Print out Header\n",
    "print('{0:>20s}{1:>12s}{2:>8s}{3:>10s}{4:>12s}'\\\n",
    "      .format('Mean', 'Variance', 'Min', 'Max', 'Non Zeroes'))\n",
    "print(65*'-')\n",
    "\n",
    "# Printout summary statistics\n",
    "for idx, (m, v, mn, mx, n) in enumerate(zip(mus, vrs, mns, mxs, nnzs)):\n",
    "    print('{5:10s}{0:10.2f}{1:12.2f}{2:8.2f}{3:10.2f}{4:12d}'\\\n",
    "          .format(m, v, mn, mx, int(n), cols[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Correlations\n",
    "\n",
    "Another useful function is to compute the correlation between different\n",
    "data sequences. The Spark MLlib package includes the `corr` method\n",
    "within the Statistics component to compute correlations between\n",
    "individual data sequences, or via the columns in an RDD. The `corr`\n",
    "method can also calculate either the _Pearson_ correlation, which is the\n",
    "default, or the _Spearman_ correlation. In the first code cell, we\n",
    "create several data sequences, turn them into Spark data structures via\n",
    "the `parallelize` method, and compute the Pearson correlation\n",
    "coefficient between the different data sequences. In the second code\n",
    "cell, we create a new RDD from three columns in the `sdt` RDD, and\n",
    "compute both the Pearson and Spearman correlations between the columns\n",
    "in this RDD.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Demonstrate Correlation Measurements\n",
    "\n",
    "# Sample Data\n",
    "x = sc.parallelize([0, 1, 2])\n",
    "y = sc.parallelize([1, 2, 4])\n",
    "z = sc.parallelize([2, 1, 0])\n",
    "\n",
    "print('x = ', x.collect())\n",
    "print('y = ', y.collect())\n",
    "print('z = ', z.collect())\n",
    "\n",
    "print('\\nPearson Correlation Tests')\n",
    "print(25*'-')\n",
    "print('x corr x = {0:+5.3f}'\\\n",
    "      .format(Statistics.corr(x, x, method='pearson')))\n",
    "\n",
    "print('x corr y = {0:+5.3f}'\\\n",
    "      .format(Statistics.corr(x, y, method='pearson')))\n",
    "\n",
    "print('x corr z = {0:+5.3f}'\\\n",
    "      .format(Statistics.corr(x, z, method='pearson')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set print precision of matrices\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "# Compute correlation of three columns in RDD\n",
    "cd = sdt.map(lambda p: (p[1], p[2], p[4]))\n",
    "\n",
    "print('Departure Time, Departure Delay, Arrival Delay')\n",
    "\n",
    "print('\\nPearson Correlation Matrix:')\n",
    "print(Statistics.corr(cd, method='pearson'))\n",
    "\n",
    "print('\\nSpearman Correlation Matrix:')\n",
    "print(Statistics.corr(cd, method='spearman'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Activities\n",
    "\n",
    "Make the following changes to see how the results change.\n",
    "\n",
    "**Note**: The hints and code insertion markings are not all-inclusive. You may need to add code before and after the hints.  They are just hints, not fully structured _fill-in-the-blank_.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity 1:** Change the `myRDD` example to start with all integers from 0 to 399. Then use an appropriate lambda function to convert this RDD to a new RDD that has all odd integers from 1 to 399."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Your code for activity 1 goes below this comment\n",
    "# ----------------------------------------------------\n",
    "\n",
    "<YourCodeHere>\n",
    "\n",
    "\n",
    "myRDD = <YourCodeHere>\n",
    "\n",
    "oddRDD = <YourCodeHere>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity 2:** Filter the previous RDD to contain only entries that are divisible by 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Your code for activity 2 goes below this comment\n",
    "# ----------------------------------------------------\n",
    "\n",
    "ninesRDD = <YourCodeHere>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity 3:** Convert this RDD to a Spark DataFrame, specify the column name as `Numbers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Your code for activity 3 goes below this comment\n",
    "# ----------------------------------------------------\n",
    "\n",
    "<YourCodeHere>\n",
    "\n",
    "df = <???>.createDataFrame(<???>)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity 4:** Change the DataFrame to include different columns from the flights data. You might review the original [airline data set](http://stat-computing.org/dataexpo/2009/) website to see the column descriptions.\n",
    " * [Make sure you are consulting the Spark Documentation](https://spark.apache.org/docs/latest/sql-programming-guide.html#dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Your code for activity 4 goes below this comment\n",
    "# ----------------------------------------------------\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity 5:** Use a SQL query on the `df` DataFrame to compute the mean distance between all flights from O'Hare to Los Angeles International Airport (LAX)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Your code for activity 5 goes below this comment\n",
    "# ----------------------------------------------------\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional, more advanced problems:\n",
    "\n",
    "**Note**: Consult the [Spark Python API](https://spark.apache.org/docs/latest/api/python/index.html) for additional help, hints, and general exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity 6:** Add an index column to the Spark DataFrame created in activity 3, which sequentially increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Your code for activity 6 goes below this comment\n",
    "# ----------------------------------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity 7:** Create an RDD containing the 'Year', 'Month', 'DayofMonth', 'dDelay',\n",
    "and 'Origin' columns for the airline data.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Your code for activity 7 goes below this comment\n",
    "# ----------------------------------------------------\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# Save your notebook\n",
    "\n",
    "**Note**: If you do not do the extra material, you should still scroll down and execute the command to release the SparkContext.\n",
    "\n",
    "# Optional/ Extra material\n",
    "\n",
    "## Machine Learning\n",
    "\n",
    "The bulk of the MLlib package is focused on performing machine learning\n",
    "at scale by using Spark. With functions for computing classification,\n",
    "regression, clustering, dimensional reduction, and more, the library\n",
    "extends considerable power to the Spark user. Since we have already\n",
    "covered these concepts by using Python and scikit-learn, in the rest of\n",
    "this Notebook, we will present two specific machine learning algorithms\n",
    "in order to demonstrate the basic concepts required to work with the\n",
    "tools in the Spark MLlib package.\n",
    "\n",
    "-----\n",
    "\n",
    "### Linear Modeling\n",
    "\n",
    "One of the simplest machine learning techniques is [linear regression][slr].\n",
    "The main difference when using Spark is that for this supervised\n",
    "learning technique our data must be in a Spark specific data structure\n",
    "called [`LabeledPoint`][slp]. Spark provides several data structures to\n",
    "simplify the application of distributed machine learning algorithms at\n",
    "scale. The labeled nature refers to the label, used for training, that\n",
    "is associated with the point. The first item in the data structure is\n",
    "the label, while the second item is the set of feature columns.\n",
    "\n",
    "In the following code cells, we first create a new data structure that\n",
    "extracts the arrival delay to be the label and the departure delay as\n",
    "the feature. These data re turned into a Spark sequence containing\n",
    "`LabeledPoint` values for each row in the original RDD. Next we display\n",
    "the first rows in the new sequence, and next we train the linear\n",
    "regressor (using SVD in this case) and specify a number of iterations\n",
    "and step size. You should feel free to modify these values and see the\n",
    "impact on the resulting performance. Finally, we compute several\n",
    "regression metrics to quantify the performance of this method on these\n",
    "data (recall that the data span a large range, hence the RMSE is quite\n",
    "reasonable).\n",
    "\n",
    "-----\n",
    "\n",
    "[slp]: https://spark.apache.org/docs/latest/mllib-data-types.html#labeled-point\n",
    "[slr]: https://spark.apache.org/docs/latest/mllib-linear-methods.html#linear-least-squares-lasso-and-ridge-regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.regression import LinearRegressionWithSGD\n",
    "from pyspark.ml.regression import LinearRegressionModel\n",
    "\n",
    "# Minimum departure delay\n",
    "min_delay = 5.\n",
    "data = fields.filter(lambda p: p[6] > min_delay).map(lambda p: LabeledPoint(p[8], [p[6]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_model = LinearRegressionWithSGD.train(data, iterations=100, step=0.00000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vnp = data.map(lambda lp: (lp.label, float(lr_model.predict(lp.features))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vnp.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "\n",
    "tm = RegressionMetrics(vnp)\n",
    "\n",
    "print('RMSE = {0:5.1f}'.format(tm.rootMeanSquaredError))\n",
    "print('MSE = {0:5.1f}'.format(tm.meanSquaredError))\n",
    "print('MAE = {0:5.1f}'.format(tm.meanAbsoluteError))\n",
    "print('r2 = {0:5.1f}'.format(tm.r2))\n",
    "print('EV = {0:5.1f}'.format(tm.explainedVariance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(lr_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional, more advanced problems:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity 8:** Add more columns into the Linear Regression demonstrated in this Notebook. In particular, include departure time and distance into the calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Your code for activity 8 goes below this comment\n",
    "# ----------------------------------------------------\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your notebook, then `File > Close and Halt`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
