{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Architecture Project\n",
    "\n",
    "\n",
    "Up to this point, you have been learning about a wide variety of cloud computing topics.\n",
    "In module 1, you learned about cloud basics, security models, virtual machines, cloud storage, public key authentication, and more.\n",
    "In moduel 2, you stood up your own Docker Container image and installed Jupyter Notebooks!  Wow!!!  \n",
    "This learning and discovery continued with the continual introduction of more and more concepts;\n",
    "seeing how components can be access through the AWS Web Console or through the `boto3` programming API.\n",
    "Module 3 explored some of the various storage options, such as DynamoDB, S3 buckets, and Redshift relational DMBS.\n",
    "Module 4 stepped things up a couple notches as you dove deeper into the concepts of servless web services, learning about Lambda functions for event-drive data transformations.\n",
    "Finally, this module you were exploring Spark and Elastic Map-Reduce clusters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's take a moment and look at some of the options for AWS services.\n",
    "\n",
    "![AWS_too_many_options.png MISSING](../images/AWS_too_many_options.png)\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can seem daunting.\n",
    "If we step back, let's recall the original goal of cloud computing.  \n",
    "**We want to have a scalable, on-demand, as-needed, computational resource that can scale with our compute and storage needs.**\n",
    "Additionally, we may not need this computing infrastruture all the time, making the \"as-needed\" a critical value.\n",
    "\n",
    "The scalability of the cloud providers, their infrastructure, and the systems we can build in the cloud make it a key to productionizing large dataset data science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at a scaled down set of AWS services\n",
    "\n",
    "![AWS-icons_2.png MISSING](../images/AWS-icons_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Compute Assessment\n",
    "\n",
    "Consider [EC2](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/concepts.html), [Lamba](https://docs.aws.amazon.com/lambda/latest/dg/welcome.html), and [EC2 Container Service](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html).\n",
    "**Specifically, read through the links.**\n",
    "\n",
    "In the cells below, please provide \n",
    "1. An assessment that compares and contrast the three compute options.\n",
    "2. What are the advantages and disadvantages of each?\n",
    "3. Conceptualize and Describe data science scenarios in which each is the best of the three alternatives."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Add your Assessment here (Task 1.1)\n",
    "# ------------------------------------\n",
    "\n",
    "All 3 services provide scalable, on demand services to users which is what we expect and desire from cloud computing services. This means that users do not need to spend time and capital creating and setting up their own servers and environments, but instead can rely on pre-packaged services from AWS. EC2 and ECS are closely related services that offer scalable servers for users developing applications. This also allows them to be pretty robust as far as what someone can create using the service. Lambda instead is more defined and boxed in, which isn't necessarily a negative, infact instead this make it better at the service it provides, which partially is the automation of data processing. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Add your advantages and disadvantages here (Task 1.2)\n",
    "# -----------------------------------------------------\n",
    "\n",
    "EC2:\n",
    "EC2 runs off the power of scalable computing environments known as instances that use preconfigured templates known as AMIs which add to simplicity by pre-packaging components needed for a server. These components include operating systems and additional hardware. There are also convenient tools and APIs to use with EC2 that make it accessible through things outside the Amazon portal should one wish to go other routes (e.g. boto3 within Python).\n",
    "Another pro is the scalability of the service and the pay plans which help to keep costs relative to what a user needs. This can help smaller business or individuals who don't have the capital to invest in server systems to create the applications they need without large upfront investments. \n",
    "A few issues with EC2 that I see are first it's owned by Amazon (3rd party with most businesses) which can cause compatibility issues with applications. What I mean is as Amazon updates and improves the EC2 system they will roll out changes that developers have to be aware of and frequently update their coding (potentially) accordingly. This could cause issues of work but also may not be a large problem. Another is that the system is moderatly complex which creates a steeper learning curve for new users. Finally, though the 'pay as needed' service is great, there is no 'grace' given if someone should forget about an instance. This could cause potential financial burden on a business or individual if an instance is forgotten and left to run when it was actually unneeded. \n",
    "\n",
    "Lambda:\n",
    "Advantages of Lambda include\n",
    "- Data processing triggers for use with things such as Amazon S3. This is one example we explored and it is clear how easy it is to create these triggers if using services like S3 to ensure data is processed as intended. These automatic triggers (which I attribute to being like SQL triggers) can make more complex processing tasks simpiler and repeatable without large code. \n",
    "- There is also an advantage with scaleable backend through AWS, which can impact performance of the Lambda functions, along with performance of other AWS compute and storage related systems. Because Lambda manages what is called the compute fleet, then it takes out the administrative needs of managing capacity, monitoring, and logging Lambda functions and useage. \n",
    "One disadvantage with Lambda that I see, even if it is minor, is you are limited to the languages that are available within the Lambda service. Although the languages in Lambda today do cover a wide assortment (i.e. Node.js, Python, Ruby, Java) there are a few key ones that are missing which could be important to some users (i.e. C, C++, C#). There is a custom Lambda setup that may get around this, but still not having some important languages ready to deploy could impact some users. \n",
    "\n",
    "EC2 Container:\n",
    "Similar to EC2, the ECS service has an advantage of being scalable with costing which is a plus for users who don't want to invest in physical servers. Another advantage is with tasks and scheduling on the ECS. This makes creating and scheduling tasks on a cluster easy which also creates flexibility for tasks and services that one can perform within their cluster. Seeing as scalability still is key with cloud services, this scheduling service takes a lot of workload off of tasks and clusters for the user. \n",
    "The main disadvantage to me, and similar to EC2, is that there is a bit of a learning curve especially when one dives deeper into what ECS can provide. Though the depth and scalability of this service is vast, that is both a postitive and negative as it means some users may not fully understand or be able to get the full extent of what the service provides. \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Add your Conceptualize and Describe here (Task 1.3)\n",
    "# ---------------------------------------------------\n",
    "\n",
    "For EC2 and ECS, both of these services seem to be primed for use with DMIR and Model building. In previous labs we used these services to create ML models, practice webscraping and downloading into SQL databases or similar. I would see using these to create scalable services as needed when creating models if the computing hardware available was not able to make the cut. For instance, if we are processing a lot of data and available systems did not have the computing hardware to handle, one could set up a EC2 instance that helps process the data and push into a ML algorithm down the pipeline. \n",
    "\n",
    "For Lambda, I would use this for auto processesing of big data from syndicated sources or unstructured APIs. As an example, setting up a Twitter API that then pre-processed recent tweets automatically before storing them in a processable format would be ideal with Lambda. We could download the latest group of relevant tweets, perform something like TF-IDF, and store in a format on S3 buckets to be used later. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Storage and Database Assessment\n",
    "\n",
    "Consider [S3](https://aws.amazon.com/s3/), [DynamoDB](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html), and [Redshift](https://docs.aws.amazon.com/redshift/latest/gsg/getting-started.html).\n",
    "**Specifically, read through the links.**\n",
    "\n",
    "In the cells below, please provide \n",
    "1. An assessment that compares and contrast the three data storage options, specifically what is the purpose of each.\n",
    "2. What are the advantages and disadvantages of each?\n",
    "3. Conceptualize and Describe data science scenarios in which each is the best of the three alternatives. Specifically, the scenario should link the data storage components to appropriate compute options from Task 1."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Add your Assessment here (Task 2.1)\n",
    "# ------------------------------------\n",
    "\n",
    "Amazon S3 is a strong object storage service that is scalable (like pretty much every service on AWS). This makes it easy for users to create 'buckets' and manage properties such as regional access and other access controls, data security, and optimizing costs with a variety of storage sizes and classes. \n",
    "\n",
    "DynamoDB is a fully managed NoSQL database service that is scalable and takes away the burden of adminstration of a NoSQL database from users. Another unique feature that is good for users is its encruption at rest feature which adds additional security to sensitive data while eliminating (or at least greatly reducing) operational burden on developers. Another feature, which is something my industry would be interested in, is its long-term retention and backup service. This is good for organizations that deal with regulatory often as it creates backups and logs of data so that you can reference sensitive data at any time even if something happens. \n",
    "\n",
    "Redshift operates as a data warehouse vs a database. This means that it is more primed and optimized for analytical queries rather than transactional queries that are more easily executed on a database (such as a NoSQL DynamoDB). Since it uses a PostfreSQL compatible querying layer, Reshift can handle more complex queries from users and retrieve data from large warehouses (even spanning millions of rows) with very fast speeds, thus optimizing it as mentioned above for analytical queries. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Add your advantages and disadvantages here (Task 2.2)\n",
    "# -----------------------------------------------------\n",
    "\n",
    "S3:\n",
    "S3 is scalable, however as we also saw if users run out of buckets it can become pricey and cause issues when needing to set up additional buckets. Users can set up Lambda functions to run with S3 buckets so take out some of the back-end work to processing data. For the most part, it is also an easy to set up service and understand as some of the UI is similar to more traditional file systems users would be familiar with. \n",
    "Some negatives include the bucket limitations and pricing, which causes it to be more expensive than other options available. Querying and searching for your data within the service is also more complex without users maintaining clean records of how buckets were partitioned and where data ends up being stored. There also (as of right now) no option to sync other drive locations with S3 (thinking similar to Google Drive and One Drive).\n",
    "\n",
    "DynamoDB:\n",
    "Scalability, and the lack of management needs are big positive sfor DynamoDB. This again takes physical burdens off users and their time and allows them to focus more on the data and how to manage/warehouse it. The encryption feature is also a plus for users who handle sensitive data and don't have the time or expertise to properly ensure security features are in place. \n",
    "For negatives, there is no contorl over hot he data is portioned from a user-end which could cause compliance issues. The costs of the service can grow as well if the user does not manage it appropriately. There also (as far as I could read) is no support for things such as triggers which would add additional complexity to data management on the user before storage. \n",
    "\n",
    "Redshit:\n",
    "Pros include faster querying using distribution keys, sort keys, and with the power of PostgreSQL behind the service itself. It also has built-in 'Analyze' operations that can be performed on the tables which further simplifies and adds analytical benefits to the service. \n",
    "A big con for Redshift is it's Automatic Vacuum sort which begins to fall apart with even larger databases of say 1 billion rows. So though this feature doesn't impact performance if a user has a small(er) database to manage, larger corperations with a lot of data may find the service lacking in expected performance.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Add your Conceptualize and Describe here (Task 2.3)\n",
    "# ---------------------------------------------------\n",
    "\n",
    "For s3, I would use this service to store and process unstructured data as similarly mentioned above. With Lambda functions this makes it easy to process some data types pre-storage to maximize how things are stored and their formats. \n",
    "\n",
    "For DynamoDB, I could see it being used to keep and store records for transactions that we may use later on. This could be good to store and process sales data for a business and keep it ready to be accessed if needed for other analytical needs. \n",
    "\n",
    "Redshift is more cost effective and offers the column-oriented database for faster queries and more analytical queries. I would use this to store and access data that we would need for more advanced and larger models such as a weather database used to predict sales down to individual retail locations across a country and entire globe. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Cloud Data Acquisition, Cleaning, and Persistence \n",
    "\n",
    "Your task is to design (not implement) a scalable data acquisition architecture that supports transformations from raw data into analytical information using AWS cloud components from Task 1 and 2, as well as an EMR cluster and/or Spark.\n",
    "Specifically, you are designing a system of AWS components that could implement the below diagram.\n",
    "\n",
    "![Data_Scraper_Structure.png MISSING](../images/Data_Scraper_Structure.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tying it all together can seem intimidating!\n",
    "\n",
    "The top sketched plot is a great anecdote of how beginners experience attempting to design data processing architectures using cloud components.\n",
    "\n",
    "![AWS_EMR_Spark.png MISSING](../images/AWS_EMR_Spark.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Design your system to consume a web API of data, **imagine live Reddit data**, assuming the data does **not** come in tabular form.\n",
    "The data should be cleaned, transformed, tidied, etc., then persisted into cloud storage (database or other).\n",
    "Machine Learning or statistical models are going process the data, producing output data/information for staging into another cloud storage component.\n",
    "The analytical storage will serve the needs of data science product consumers (managers, sales teams, etc.)\n",
    "\n",
    "\n",
    "### Task 3.1\n",
    "\n",
    "Use a digital drawing software (Powerpoint, Google Draw, Draw.io, etc.) to design a diagram of components and connectivity.\n",
    "Save the image as either `StudentDesign.png` or `StudentDesign.jpg` and upload into the `module5/exercises` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Update the image name to .jpg if you created a JPEG instead of PNG.\n",
    "This <span style=\"background:yellow\">should be a MISSING image until</span> you have completed this step and run the cell to render the image.\n",
    "\n",
    "---\n",
    "\n",
    "![StudentDesign.png MISSING](./StudentDesign2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### Task 3.2\n",
    "\n",
    "Provide a written description of your architecture design.  For **each component** that is part of the overall structure detail the following.\n",
    "1. Why the component was chosen over other potential components of the similar usage (e.g., dyanamo vs redshift)\n",
    "1. What is the incoming data?\n",
    "1. What invokes the processing of the data in a timely manner?\n",
    "1. What is the output format and location of from the component.\n",
    "\n",
    "Finally, Provide an assessment of the number of hours you estimate it would take you to implement your design."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Your answer goes here (Task 3.2)\n",
    "# --------------------------------\n",
    "\n",
    "First, data is collected through a Reddit API bringing in Posts stored as unstructured data in S3. Lambda function then operates to pre-process the data for easier usage down-stream. \n",
    "\n",
    "Next, EC2 service picks up the pre-processed data to run additional processes on it such as vectorizing the text data to process topics of posts. \n",
    "\n",
    "These are then stored in Redshift for better scalability and faster analytical processing vs storing in DynamoDB. This also allows us to query faster results if we want to pull out identified topics and get all text or specific pieces of posts for certain topics. \n",
    "\n",
    "Finally, we can call ECS to run more advanced analytics such as sentiment scores, or maybe predictions of post scores and number of comments it will receive within a timeframe. \n",
    "\n",
    "\n",
    "With the comfort level I have in the AWS services and my previous knowledge I have in the Reddit API and various text-transforming methods, I would estimate that this service would take me 20-30 hours to initially develop, and an extra 10-20 hours to fine-tune as we run it a few times and see if it works. Since I have never done this before though, I could expect to be on the high end if not surpassing this time. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### Task 4.1\n",
    "\n",
    "Consider the `Jupyter_from_Docker` in module 2 labs, as well as the Apache Spark and EMR labs in module 5. \n",
    "Describe how you would use an [Allspark](https://hub.docker.com/r/jupyter/all-spark-notebook/) container in AWS, collaborating with an EMR cluster to faciliate analytics over the data that you persist in your cloud architecture design. How does this support scalability?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Your answer goes here (Task 4.1)\n",
    "# --------------------------------\n",
    "\n",
    "From my understanding of the PySpark/AllSpark labs, I would create the container with the EMR cluster to improve not only scalability but also the speed and processing power of the data that is stored through the PySpark application. Since we would use containers and nodes this would allow the processes we have set up (i.e. as example the Reddit data) to all run more efficiently through the nodes we set up in EMR. \n",
    "\n",
    "Additionally, after playing with it, this also takes complexity out of operating systems and environments as we can distribute this code out amongst the EMR cluster and not worry about what services are on a local machine (i.e. java). This also helps with scalability as additional setup is not necessary for each instance as we rather just expand our service and access. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### Task 4.2\n",
    "\n",
    "Think of the lab Apache Spark. \n",
    "What do you need to scale to a larger data? \n",
    "Review this [website](https://files.pushshift.io/reddit/comments/) where you can find the same data of the Apache Spark practice, but much more data.\n",
    "Describe your strategy to analyze all the data available at the link above.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Your answer goes here (Task 4.2)\n",
    "# --------------------------------\n",
    "\n",
    "Since in spark we can scale the number of needed nodes from the minimum to maximum, we can start with the smallest workload and then move up as we add more tasks and data. \n",
    "\n",
    "My strategy to analyze the larger amount of data would be to start with smaller files, refining the process as I go with that to improve learnings and runtime. Once the process has been vetted and we can begin to scale, we can increase the needed nodes in the service to import more data and process it faster on the expanded cluster. This scales again not only the Apache Spark service we have set up, but also our own process power and understanding of how the system works and what benefits we gain from storing and analyzing bigger datasets on the Apache Spark service. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"background:yellow\">Warning: Do not try to implement this now! This is only a design exercise.</span>\n",
    "\n",
    "\n",
    "# Save your notebook, then `File > Close and Halt`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
